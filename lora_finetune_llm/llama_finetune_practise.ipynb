{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from math import ceil\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_8bit = True,)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "                                             quantization_config = quantization_config, \n",
    "                                             token = os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"pcuenq/Llama-3.2-1B-Instruct-tokenizer\", \n",
    "                                          trust_remote_code = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 6861\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(path = \"aboonaji/wiki_medical_terms_llam2_format\", split = \"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = dataset.num_rows\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(output_dir = \"./llama_finetune\", \n",
    "                         per_device_train_batch_size = batch_size, \n",
    "                         max_steps = 5 * (ceil(sample_size/batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(model = model, args = args, \n",
    "                     train_dataset = dataset,\n",
    "                     tokenizer = tokenizer, \n",
    "                     peft_config = LoraConfig(task_type = \"CAUSAL_LM\", r = 128, lora_alpha = 16, lora_dropout = .1, ),\n",
    "                     dataset_text_field = \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhawkiyc\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/revlis_ai/Documents/temp/wandb/run-20241018_180321-4g30fwyb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hawkiyc/huggingface/runs/4g30fwyb' target=\"_blank\">./llama_finetune</a></strong> to <a href='https://wandb.ai/hawkiyc/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hawkiyc/huggingface' target=\"_blank\">https://wandb.ai/hawkiyc/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hawkiyc/huggingface/runs/4g30fwyb' target=\"_blank\">https://wandb.ai/hawkiyc/huggingface/runs/4g30fwyb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8580 [00:00<?, ?it/s]/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  6%|▌         | 500/8580 [11:29<3:12:31,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8533, 'grad_norm': 0.13843896985054016, 'learning_rate': 4.708624708624709e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67123521-330efa8f784928e91f009c6f;d10c12e3-bf19-4427-bcf6-c52a579c525d)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▏        | 1000/8580 [23:04<2:52:11,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7424, 'grad_norm': 0.17795269191265106, 'learning_rate': 4.4172494172494175e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-671237d9-160752b211a270dc6a23a11a;c1b4e7f5-acbd-49e5-a4e0-77fd0c9ac4e7)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 17%|█▋        | 1500/8580 [34:37<2:44:39,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7314, 'grad_norm': 0.19075030088424683, 'learning_rate': 4.125874125874126e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67123a8e-2d7505ee08a72fa273b443b4;2420bfe3-0895-4f32-b372-9891e6c2b4b4)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 23%|██▎       | 2000/8580 [46:16<2:34:03,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7516, 'grad_norm': 0.26657453179359436, 'learning_rate': 3.834498834498835e-05, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67123d49-40926179445812b517287ffd;4d427613-908c-44b0-87d6-984ad3391e70)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 29%|██▉       | 2500/8580 [57:51<2:23:18,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7234, 'grad_norm': 0.18379968404769897, 'learning_rate': 3.5431235431235434e-05, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67124000-3365a1e861362542341222f6;fc255431-8650-4f24-9bf8-8fc01f697e3d)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 35%|███▍      | 3000/8580 [1:09:24<2:08:59,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7201, 'grad_norm': 0.18118135631084442, 'learning_rate': 3.251748251748252e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-671242b5-37a4be3e74b0c32716e70947;3e952845-d132-4a4c-828f-a5569940fa7c)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 41%|████      | 3500/8580 [1:20:56<2:00:38,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7078, 'grad_norm': 0.1875627189874649, 'learning_rate': 2.9603729603729606e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67124569-5b4b4954772524b878b3d549;6d7ee13d-75ec-4cb5-8f00-2e90b6c685de)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 47%|████▋     | 4000/8580 [1:32:30<1:49:39,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7167, 'grad_norm': 0.19575345516204834, 'learning_rate': 2.6689976689976692e-05, 'epoch': 2.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6712481f-0f3f436f24513748386f4ac7;aaa3747b-f90f-47fe-9c6d-28a59e327519)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 52%|█████▏    | 4500/8580 [1:44:09<1:36:21,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7323, 'grad_norm': 0.22222746908664703, 'learning_rate': 2.377622377622378e-05, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67124ad9-670132674e906081437bfa23;a8f8a5be-ccd2-4eae-8c42-f26aefbac5a9)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 58%|█████▊    | 5000/8580 [1:55:44<1:23:10,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6974, 'grad_norm': 0.22694863379001617, 'learning_rate': 2.0862470862470865e-05, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67124d91-0e7aef9d009f0ad729b93e2d;e2b9872e-1669-4827-b178-57d135eb0ad4)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 64%|██████▍   | 5500/8580 [2:07:13<54:49,  1.07s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6982, 'grad_norm': 0.3130342662334442, 'learning_rate': 1.794871794871795e-05, 'epoch': 3.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67125042-3f636a3540f5fd3f77b4b1b9;b7c914b6-6031-4103-9105-a9bc6468a387)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 70%|██████▉   | 6000/8580 [2:18:53<1:01:06,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7119, 'grad_norm': 0.17226332426071167, 'learning_rate': 1.5034965034965034e-05, 'epoch': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-671252fe-447199e37bce19230479fcaa;6c6b7fe9-114a-4c2c-862f-93824c75f59e)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 76%|███████▌  | 6500/8580 [2:30:34<49:46,  1.44s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7183, 'grad_norm': 0.18550890684127808, 'learning_rate': 1.2121212121212122e-05, 'epoch': 3.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-671255bb-14a544f761119e0d6de57216;f09d1df1-abb7-4037-85d7-b6da637635e2)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 82%|████████▏ | 7000/8580 [2:42:09<37:32,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7166, 'grad_norm': 0.2062445878982544, 'learning_rate': 9.207459207459208e-06, 'epoch': 4.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67125872-40e7751939a659680f8a9fe5;1841f2df-f830-4939-9209-8a8071bbbfa2)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 87%|████████▋ | 7500/8580 [2:53:37<25:38,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6913, 'grad_norm': 0.2218349725008011, 'learning_rate': 6.2937062937062944e-06, 'epoch': 4.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67125b22-3ceda243529d78da6c10715b;9c321b8f-5585-460e-9ee7-3032bfe5a5e5)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 93%|█████████▎| 8000/8580 [3:05:17<13:06,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7189, 'grad_norm': 0.2134796380996704, 'learning_rate': 3.3799533799533803e-06, 'epoch': 4.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67125dde-3de5f327019f145362e20358;69f96d1d-abb0-40f6-a89d-33767f68acaa)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 99%|█████████▉| 8500/8580 [3:16:52<01:54,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7083, 'grad_norm': 0.18375183641910553, 'learning_rate': 4.662004662004662e-07, 'epoch': 4.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67126095-28fda80c782419166e4e62fc;ab082d27-0480-4362-b026-9a39178f0cd4)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|██████████| 8580/8580 [3:18:40<00:00,  1.12s/it]/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67126101-45dceeae403446c77c4707d9;d5bde104-54cc-4f18-b6a3-e91c075eb92a)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "100%|██████████| 8580/8580 [3:18:41<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11929.895, 'train_samples_per_second': 2.877, 'train_steps_per_second': 0.719, 'train_loss': 1.725535412768384, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8580, training_loss=1.725535412768384, metrics={'train_runtime': 11929.895, 'train_samples_per_second': 2.877, 'train_steps_per_second': 0.719, 'total_flos': 2.01700558334976e+17, 'train_loss': 1.725535412768384, 'epoch': 5.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/revlis_ai/anaconda3/envs/env_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What is malaria? [/INST] Malaria is a serious and sometimes life-threatening disease caused by a parasite of the Plasmodium genus. The parasite infects red blood cells and causes the red blood cells to die. The disease is spread by the bite of an infected female Anopheles mosquito. Symptoms include fever, chills, flu-like symptoms, and in severe cases, a headache, vomiting, and in some cases, kidney failure and death. Malaria is a major public health problem in tropical and subtropical regions of the world, with an estimated 228 million cases and 435,000 deaths in 2019.\n",
      "The disease is caused by the Plasmodium species P. vivax, P. ovale, P. malariae, and P. falciparum, which are all transmitted by the Anopheles mosquito. The parasite is transmitted through the bite of an infected female Anopheles mosquito that feeds on the blood of an infected individual, usually after 10 days of infection. The parasite multiplies in the red blood cells of the host, and the infected red blood cells are then destroyed by the host's immune system. The parasite is transmitted by the bite of an infected female Anopheles mosquito that feeds on the blood of an infected individual, usually after 10 days of infection. The parasite multiplies in the red blood cells of the host, and the infected red blood cells are then destroyed by the host's immune\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is malaria?\"\n",
    "text_generation_pipeline = pipeline(task = \"text-generation\", model = model, tokenizer = tokenizer, max_length = 300)\n",
    "model_answer = text_generation_pipeline(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(model_answer[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Please tell me about Bursitis [/INST] Bursitis is inflammation of a bursa, a fluid-filled sac that cushions a joint, tendon, or muscle. Bursitis is often caused by repetitive motion, injury, or infection. It can be a chronic or acute condition. Bursitis is usually painless and can be associated with other conditions such as arthritis. Bursitis is a common cause of joint pain and can be treated with medication, physical therapy, or surgery.\n",
      "Inflammation of the bursa, the bursa is a fluid-filled sac that cushions a joint, tendon, or muscle. The bursa is filled with synovial fluid, a fluid that is produced by the synovial membrane. The bursa is located in the area of the joint, tendon, or muscle that is subjected to repetitive motion. The bursa is a protective layer that reduces friction between the joint, tendon, or muscle and the bone. When the bursa becomes inflamed, the friction between the joint, tendon, or muscle and the bone increases, causing pain and inflammation.\n",
      "Bursitis can be acute or chronic. Acute bursitis is a sudden onset of inflammation that can be caused by injury, infection, or repetitive motion. Chronic bursitis is a long-term inflammation that can be caused by repetitive motion, injury, or infection. Chronic bursitis can cause pain and stiffness in the affected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Please tell me about Bursitis\"\n",
    "text_generation_pipeline = pipeline(task = \"text-generation\", model = model, tokenizer = tokenizer, max_length = 300)\n",
    "model_answer = text_generation_pipeline(f\"<s>[INST] {user_prompt} [/INST]\")\n",
    "print(model_answer[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "env_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
