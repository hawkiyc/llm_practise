{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(api_key=api_key,model=\"gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = '''\n",
    "    Four score and seven years ago our fathers brought forth, upon this continent, a new nation, conceived in liberty, and dedicated to the proposition that all men are created equal.\n",
    "    Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived, and so dedicated, can long endure. We are met on a great battle field of that war. We come to dedicate a portion of it, as a final resting place for those who died here, that the nation might live. This we may, in all propriety do.\n",
    "    But, in a larger sense, we can not dedicate we can not consecrate we can not hallow, this ground The brave men, living and dead, who struggled here, have hallowed it, far above our poor power to add or detract. The world will little note, nor long remember what we say here; while it can never forget what they did here.\n",
    "    It is rather for us, the living, we here be dedicated to the great task remaining before us that, from these honored dead we take increased devotion to that cause for which they here, gave the last full measure of devotion that we here highly resolve these dead shall not have died in vain; that the nation, shall have a new birth of freedom, and that government of the people, by the people, for the people, shall not perish from the earth.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages = [SystemMessage(content='You are an AI assistant specializing in summarizing speeches.'),\n",
    "                 HumanMessage(content=f'Please provide a short and concise summary by given speech:/n Text:{speech}'),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33646/1441627176.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  llm(chat_messages).content\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is Abraham Lincoln\\'s Gettysburg Address. \\n\\nDelivered at the dedication of a national cemetery, Lincoln framed the Civil War as a test of whether a nation \"conceived in liberty, and dedicated to the proposition that all men are created equal\" could survive.  \\n\\nHe emphasized that the battlefield itself was already hallowed by the sacrifices of the soldiers, and urged the living to dedicate themselves to the unfinished work of preserving the Union and ensuring that \"government of the people, by the people, for the people, shall not perish from this earth.\" \\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(chat_messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='This is the Gettysburg Address by President Abraham Lincoln. \\n\\nLincoln commemorates the fallen soldiers at Gettysburg, emphasizing that their sacrifices have already consecrated the battlefield far beyond any words he could speak. He calls upon the living to dedicate themselves to the unfinished work of preserving the Union and ensuring that \"government of the people, by the people, for the people, shall not perish from this earth.\" \\n\\n\\nThe speech is a powerful reminder of the cost of freedom and a call to action for the nation to continue striving for a more perfect union. \\n\\n', response_metadata={'token_usage': {'completion_tokens': 113, 'prompt_tokens': 321, 'total_tokens': 434, 'completion_time': 0.205454545, 'prompt_time': 0.017338029, 'queue_time': 0.026391546999999998, 'total_time': 0.222792574}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-b600b21e-2e77-4f9c-8a99-8ba3af3b7d90-0', usage_metadata={'input_tokens': 321, 'output_tokens': 113, 'total_tokens': 434})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(chat_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Template Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    Write a summary of the following speech:\n",
    "    Speech: {speech}\n",
    "    Translate the summary into {language}.\n",
    "    \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"speech\",\"language\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Write a summary of the following speech:\\n    Speech: \\n    Four score and seven years ago our fathers brought forth, upon this continent, a new nation, conceived in liberty, and dedicated to the proposition that all men are created equal.\\n    Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived, and so dedicated, can long endure. We are met on a great battle field of that war. We come to dedicate a portion of it, as a final resting place for those who died here, that the nation might live. This we may, in all propriety do.\\n    But, in a larger sense, we can not dedicate we can not consecrate we can not hallow, this ground The brave men, living and dead, who struggled here, have hallowed it, far above our poor power to add or detract. The world will little note, nor long remember what we say here; while it can never forget what they did here.\\n    It is rather for us, the living, we here be dedicated to the great task remaining before us that, from these honored dead we take increased devotion to that cause for which they here, gave the last full measure of devotion that we here highly resolve these dead shall not have died in vain; that the nation, shall have a new birth of freedom, and that government of the people, by the people, for the people, shall not perish from the earth.\\n    \\n    Translate the summary into Chinese.\\n    '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_prompt = prompt.format(speech=speech, language=\"Chinese\")\n",
    "complete_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(complete_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33646/742969981.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  llm_chain = LLMChain(llm = llm, prompt = prompt)\n",
      "/tmp/ipykernel_33646/742969981.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  summary = llm_chain.run({\"speech\":speech, \"language\":\"Chinese\"})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'## Summary of the Gettysburg Address:\\n\\nThis speech, delivered by President Abraham Lincoln at the dedication of a national cemetery at Gettysburg, Pennsylvania, reflects on the Civil War and the meaning of freedom. \\n\\nLincoln reminds the audience that the nation was founded on the principles of liberty and equality for all. He acknowledges the immense sacrifice made by the soldiers who fought and died at Gettysburg, stating that their actions have already consecrated the ground, far beyond the power of words. \\n\\nThe speech then shifts its focus to the living, calling upon them to dedicate themselves to the unfinished work of preserving the Union and ensuring that the sacrifices made at Gettysburg were not in vain. Lincoln emphasizes the need to rededicate themselves to the cause of freedom and to ensure that \"government of the people, by the people, for the people, shall not perish from the earth.\"\\n\\n\\n##  中文翻译：\\n\\n**Gettysburg演说概要：**\\n\\n这番演说是由亚伯拉罕·林肯总统在宾夕法尼亚州盖茨堡国立墓地的落成典礼上发表的，它反思了内战以及自由的意义。\\n\\n林肯提醒听众，这个国家建立在自由和所有人平等的原则之上。他承认在盖茨堡作战和牺牲的士兵做出的巨大牺牲，并指出他们的行动已经使这片土地神圣化，远远超出了语言的力量。\\n\\n演说随后将重点转向活着的人，呼吁他们致力于保存联盟的未完成工作，并确保在盖茨堡做出的牺牲没有白费。林肯强调必须重新致力于自由事业，并确保“人民的政府，由人民、为人民”不会从地球上消失。 \\n\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm = llm, prompt = prompt)\n",
    "summary = llm_chain.run({\"speech\":speech, \"language\":\"Chinese\"})\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stuff Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 0}, page_content='JoLT: Jointly Learned Representations of\\nLanguage and Time-Series\\nYifu Cai, Mononito Goswami, Arjun Choudhry, Arvind Srinivasan, Artur Dubrawski\\nAuton Lab, School of Computer Science, Carnegie Mellon University\\nPittsburgh, PA 15213\\narvind.srini.8@gmail.com ,{yifuc, mgoswami, arjuncho, awd}@andrew.cmu.edu\\nAbstract\\nTime-series and text data is prevalent in healthcare and frequently exist in tandem,\\nfor e.g., in electrocardiogram (ECG) interpretation reports. Yet, these modalities\\nare typically modeled independently. Even studies that jointly model time-series\\nand text do so by converting time-series to images or graphs. We hypothesize\\nthat explicitly modeling time-series jointly with text can improve tasks such as\\nsummarization and question answering for time-series data, which have received\\nlittle attention so far. To address this gap, we introduce JoLT to jointly learn desired\\nrepresentations from pre-trained time-series and text models. JoLT utilizes a\\nQuerying Transformer (Q-Former) to align the time-series and text representations.\\nOur experiments on a large real-world electrocardiography dataset for medical time-\\nseries summarization show that JoLT outperforms state-of-the-art image captioning\\nand medical question-answering approaches, and that the decoder architecture, size,\\nand pre-training data can vary the performance on said tasks.\\n1 Introduction\\nTime-series and text data are frequently recorded in routine clinical care. But unlike general text\\nor time-series, clinical data can only be analyzed by medical professionals, who spend substantial\\namounts of time analyzing biosignals, and entering summaries into electronic health records, away\\nfrom direct patient care.\\nTo cater to an ever-increasing need to effectively and efficiently interpret clinical waveforms and text\\ndata, numerous studies have been devoted to automating clinical time-series and text interpretation.\\nHowever, existing studies suffer from three key limitations. First, most existing studies model time-\\nseries and text independently, even when these modalities frequently co-exist, e.g., electrocardiogram\\n(ECG) and clinical description of findings. Second, the few studies that jointly model time-series\\nand text are primarily rule-based, and do not offer the fluency and versatility associated with neural\\napproaches. Third, most existing multi-modal methods do not explicitly model time-series data,\\ninstead converting it to graphs or images and using graph or computer vision models, respectively.\\nWe introduce JoLT ,Jointly Learned Representations of Language and Time-series, a neural model\\nwhich can generate text given time-series and textual prompts as input. We evaluate JoLT on a\\nmedical time-series summarization problem on the PTB-XL dataset, and compare it with a state-of-\\nthe-art image captioning model, BLIP-2 [ 1]. To the best of our knowledge, JoLT is one of the first\\nautomated ECG interpretation methods that explicitly models time-series to generate meaningful\\ntextual interpretations. Our experiments show that explicitly modeling time-series data can improve\\ntime-series summarization performance over state-of-the-art approaches pre-trained on vast amounts\\nof data, and can enable tasks that can be obscure for time-series and text multi-modal data, like\\nquestion answering.\\n1st Workshop on Deep Generative Models for Health at NeurIPS 2023.'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 1}, page_content='Time-series\\nEncoderQ-Former\\nT ext...Language\\nModelGenerative Learning\\nsinus rhythm type left\\nbundle branch block\\nQueries\\nRepresentation\\nLearningTime-series Pre-trainingFigure 1: An overview of JoLT . Given a time-series and an optional textual prompt as input, JoLT\\nproduces text as output. We pre-train a Transformer using the masked time-series reconstruction\\nobjective to use as an encoder, and the pre-trained language model as the decoder. The Q-Former\\nis trained to align time-series and text representations. Learnable query tokens are used to extract\\ntime-series features conditioned on textual prompts.\\n2 Related Work\\nTime-series and Text Multimodal Models. Numerous studies have explored the problem of learning\\nmultimodal representations of data, such as graphs [ 2], image [ 1], and tabular data [ 3], grounded in\\ntext [ 4]. However, the challenge of jointly modeling time-series and text data has been relatively\\nunexplored, primarily due to the lack of large publicly available paired time-series and text (pre-)\\ntraining data, i.e., there is no equivalent of LAION-5B [ 5] or MS-COCO [ 6]. On the contrary, most\\nexisting time-series datasets are domain-specific, e.g. ECG interpretation [ 7] or stock price variations.\\nThis is exacerbated by the fact that most existing models are either statistical or rule-based, and\\nnecessitating substantial domain expertise that does not readily transfer across different domains [ 8].\\nClinical Text Summarization. In the healthcare domain, numerous studies have underscored the\\nimportance of developing automated text summarization systems. For instance, [ 9] highlights the\\npressing need for automated clinical report generation to alleviate the time burden on medical\\nprofessionals, allowing them to focus more on patient care. Another relevant work by Harris and Zaki\\n[10] introduced a CNN-LSTM framework designed to generate summaries of personal health data,\\nsuch as heart rate, step count, and nutrient intake. Their approach demonstrated reasonably good\\nperformance in this context. However, it is worth noting that the quality of the generated summaries\\nheavily relied on the paired training texts, which were created using rule-based methods. We expect\\nneural methods to outperform neural methods relying on rule-based methods.\\n3 Problem Formulation and Methods\\n3.0.1 Time-series Summarization.\\nGiven a time-series T ∈RC×Lof length LwithCchannels, our goal is to generate a textual\\ninterpretation of salient time-series features in the context of a target domain.\\n3.0.2 Model.\\nJoLT comprises of a time-series encoder, a text decoder, and a transformer model which ties these\\ntwo unimodal components together (Fig. 1) [ 11]. The time-series encoder is a transformer model\\nwhich treats time-series sub-sequences as input tokens. Our best model uses the Open Pre-trained\\nTrained (OPT) language model as a decoder, although we also evaluate the model with various other\\ndecoder models[ 12]. To align time-series and text representations, we leverage Querying Transformer\\n(Q-Former) introduced by Li et al. [1].\\nPre-training Time-series Encoder. First, we break the input time-series into disjoint sub-sequences\\ncalled patches. A small percentage of these patches are masked uniformly at random and then fed\\ninto the encoder, which is trained to reconstruct the masked patches using the Mean Squared Error\\nloss.\\nRepresentation Learning. In this stage, we freeze the pre-trained encoder and train the Q-Former\\nto learn query embeddings that capture salient time-series representations that are informative of\\n2'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 2}, page_content='sinus rhythm position type normal left bundle branch block left\\nhypertrophy possible 4.46 unconfirmed reportGround\\nTruth\\nsinus rhythm. normal ecgFine-tuned\\nBLIP-2\\nsinus rhythm left type left bundle branch block left hypertrophy\\npossible 4.46 unconfirmed reportJoLT\\n(OPT-2.7B)\\nTable 1: Qualitative evaluation of the results generated by JoLT compared to fine-tuned BLIP-2.\\nJoLT (with OPT-2.7B decoder) generates text summaries very similar to the ground truth, while the\\nfine-tuned BLIP-2 got the base class correct, but incorrectly described the time-series sample.\\ninput text. The Q-Former is trained using three objectives: (1) a contrastive loss to align time-series\\nand text representations by maximizing their mutual information, (2) a text generation loss to train\\nthe Q-Former to generate text conditioned on input time-series, and a (3) time-series text matching\\nlossfor finer grained alignment between time-series and text representations.\\nGenerative Learning. In this stage, we finally connect the frozen time-series encoder and Q-\\nFormer, with the frozen decoder, to leverage its generative capability. The query embeddings serve as\\nsoft prompts to guide the decoder’s language generation. We train the model end-to-end using the\\ncausal language modeling loss.\\n4 Case Study: ECG Interpretation\\nDataset. We conduct an experiment on the PTB-XL dataset [ 7] to evaluate JoLT ’s ability to\\ngenerate meaningful clinical interpretations from ECG waveform data. The dataset comprises of\\n21,837 12-lead, 10 seconds long ECG recordings collected from 18,885 patients. A subset of ECG\\nrecordings is paired with gold-standard clinical interpretation, which we use to train and fine-tune\\nour model. The train, validation, and test sets contain 11,319, 1,636, and 1,650 samples of paired\\ntime-series and text, respectively.\\nExperimental Setup. We compare JoLT with the state-of-the-art image captioning model BLIP-2\\nas baseline. We use the Matplotlib package1to transform time-series into graphical images before\\nfeeding them into BLIP-2 . We evaluate BLIP-2 in a zero-shot setting. Since the models are not\\npre-trained on medical data, we also compare JoLT with BLIP-2 fine-tuned on the PTB-XL dataset.\\nWe evaluate multiple metrics that are commonly used to evaluate text generation performance.\\nWe further run ablation experiments to evaluate the impact of the decoder on our model’s performance.\\nSpecifically, we evaluate different architectures (e.g. GPT-2-Large versus OPT), of different sizes\\n(e.g. OPT-2.7B versus OPT-6.7B ), and pre-trained on different data (e.g. BioGPT-Large versus\\nGPT-2-Large ). Beyond ECG interpretation, we also run preliminary experiments to explore our\\nJoLT ’s ability to solve multiple choice questions conditioned on time-series, on the PTB-XL dataset.\\nTo the best of our knowledge, this unique but important problem has not been explored in prior work.\\nWe compare JoLT against BiomedCLIP [13] as a baseline.\\nTables 1, 2, 3, and 4 summarize the results of our experiments. Below, we highlight some key\\nobservations.\\nDomain-specific fine-tuning is critical for clinical waveform interpretation. Poor performance\\nof off-the-shelf BLIP-2 with respect to its fine-tuned counterpart shows the need for domain-specific\\nfine-tuning, at least within the clinical domain. This motivates the need for publicly available large\\npaired time-series and text datasets and models.\\n1https://pypi.org/project/matplotlib/\\n3'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 3}, page_content='Model Fine-tuned Rouge-1 Rouge-2 Rouge-LMETEOR BLEURTR P F 1 R P F 1 R P F 1\\nBLIP-2 × 0.014 0.027 0.016 0.000 0.001 0.001 0.014 0.026 0.016 0.048 -1.283\\nBLIP-2 ✓ 0.217 0.343 0.227 0.100 0.193 0.107 0.215 0.341 0.225 0.202 -0.930\\nJoLT (OPT-2.7B) ✓ 0.404 0.528 0.436 0.277 0.355 0.295 0.403 0.526 0.435 0.414 -0.502\\nTable 2: JoLT (with OPT-2.7B decoder) outperforms zero-shot and fine-tuned BLIP-2 baselines\\nfor the ECG interpretation task on the PTB-XL dataset. R,P, and F1denote the Recall, Precision,\\nand F 1score.\\nDecoder Rouge-1 Rouge-2 Rouge-LMETEOR BLEURTR P F 1 R P F 1 R P F 1\\nOPT-2.7B 0.404 0.528 0.436 0.277 0.355 0.295 0.403 0.526 0.435 0.414 -0.502\\nOPT-6.7B 0.400 0.518 0.429 0.277 0.350 0.294 0.399 0.517 0.428 0.408 -0.499\\nGPT-2 0.113 0.420 0.169 0.020 0.080 0.029 0.113 0.419 0.168 0.017 -0.885\\nBioGPT 0.107 0.342 0.153 0.022 0.072 0.031 0.107 0.342 0.153 0.212 -1.122\\nBioMedLM 0.118 0.390 0.164 0.003 0.009 0.004 0.112 0.380 0.158 0.136 -1.079\\nTable 3: JoLT with OPT-2.7B decoder outperforms JoLT with other decoders with different pre-\\ntrained data, different sizes, and different architectures on the ECG interpretation task on the PTB-XL\\ndataset. R,P, and F1denote the Recall, Precision, and F 1score.\\nModel Accuracy Weighted Precision Weighted Recall Weighted F1Score\\nBiomedCLIP 0.11 0.57 0.11 0.02\\nJoLT (OPT-2.7B) 0.54 0.31 0.54 0.4\\nTable 4: Time-series conditioned multiple-choice question answering results on accuracy, precision,\\nrecall and F1-score. JoLT substantially outperforms BiomedCLIP , when given the prompt “ Which of\\nthe following five diagnostic classes does the following ECG belong to? \". Weighted averages are\\nmeasured with respect to the support for each class.\\nExplicitly modeling time-series improves summarization performance. JoLT produces textual\\nsummaries which are closer to ground truth compared to BLIP-2. We believe that the difference in\\nperformance largely stems from JoLT ’s ability to capture salient time-series features.\\nTime-series and text joint modeling helps improve upon baselines for multi-class question\\nanswering. JoLT outperforms BiomedCLIP on time-series conditioned multiple choice question\\nanswering problem. However, we note that both the models perform poorly on this task. We believe\\nthat future work should carefully look to improve our model’s performance on this important task,\\nwith both models overconfidently predicting the majority class.\\nBroken assumptions. For our ablation experiments, we hypothesized that: (1) Language models\\nwith more parameters will be better at text generation than smaller models, and (2) models pre-trained\\non clinical data ( BioGPT ,BiomedLM ) will be better than those trained on general text data ( GPT-2 ).\\nHowever, our experiments did not support any of these hypothesis. The former can be partly explained\\nby the fact that clinical interpretations are terse and do not require fluent large language models. We\\nbelieve that further experiments are necessary to conclusively accept or reject the latter hypothesis.\\n5 Conclusion and Future Work\\nIn this work, we introduced JoLT , a jointly model text and time-series data with a focus on ECG\\ninterpretation. We evaluated our model against state-of-the-art image captioning models in the context\\nof clinical summarization. It’s worth noting a crucial aspect of our approach: the encoder of JoLT\\nwas pre-trained using a relatively small set of time-series data from the PTB-XL dataset. We posit\\nthat extending this pre-training phase to include a large amount of time-series data is likely to further\\nimprove its performance. Additionally, it’s important to acknowledge that the evaluation of these\\nmodels presents significant challenges, as discussed in prior research [ 14]. Therefore, future research\\nendeavors should aim to comprehensively and robustly assess the capabilities of such models in\\nclinical applications.\\n4'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 4}, page_content='6 Acknowledgments\\nThis work was partially supported by the U.S. Army Research Office and the U.S. Army Futures\\nCommand under contract W911NF-20-F-0020.\\nReferences\\n[1]Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image\\npre-training with frozen image encoders and large language models. In ICML , 2023.\\n[2]Anthony Colas, Mehrdad Alvandipour, and Daisy Zhe Wang. GAP: A graph-aware lan-\\nguage model framework for knowledge graph-to-text generation. In Proceedings of the 29th\\nInternational Conference on Computational Linguistics , pages 5755–5769, Gyeongju, Repub-\\nlic of Korea, October 2022. International Committee on Computational Linguistics. URL\\nhttps://aclanthology.org/2022.coling-1.506 .\\n[3]Rong Ye, Wenxian Shi, Hao Zhou, Zhongyu Wei, and Lei Li. Variational template machine for\\ndata-to-text generation. In International Conference on Learning Representations , 2020. URL\\nhttps://openreview.net/forum?id=HkejNgBtPB .\\n[4]Mandar Sharma, Ajay Gogineni, and Naren Ramakrishnan. Innovations in neural data-to-text\\ngeneration: A survey, 2023.\\n[5]Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\\n5b: An open large-scale dataset for training next generation image-text models. Advances in\\nNeural Information Processing Systems , 35:25278–25294, 2022.\\n[6]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\\nVision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\\nProceedings, Part V 13 , pages 740–755. Springer, 2014.\\n[7]Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Dieter Kreiseler, Fatima I. Lunze,\\nWojciech Samek, and Tobias Schaeffter. Ptb-xl, a large publicly available electrocardiography\\ndataset. Scientific Data , 2020. doi: https://doi.org/10.1038/s41597-020-0495-6.\\n[8]Siddharth Biswal, Cao Xiao, M. Brandon Westover, and Jimeng Sun. Eegtotext: Learning\\nto write medical reports from eeg recordings. In Finale Doshi-Velez, Jim Fackler, Ken Jung,\\nDavid Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiens, editors, Proceedings of the\\n4th Machine Learning for Healthcare Conference , volume 106 of Proceedings of Machine\\nLearning Research , pages 513–531. PMLR, 09–10 Aug 2019. URL https://proceedings.\\nmlr.press/v106/biswal19a.html .\\n[9]Steffen Pauws, Albert Gatt, Emiel Krahmer, and Ehud Reiter. Making effective use of healthcare\\ndata using data-to-text technology, 2018.\\n[10] Jonathan Harris and Mohammed J. Zaki. Towards neural numeric-to-text generation from\\ntemporal personal health data, 2022.\\n[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁ ukasz Kaiser, and Illia Polosukhin. Transformers: Attention is all you need. In I. Guyon,\\nU. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, ed-\\nitors, Advances in Neural Information Processing Systems , volume 30. Curran Associates,\\nInc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/\\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .\\n[12] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam\\nShleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke\\nZettlemoyer. Opt: Open pre-trained transformer language models, 2022.\\n5'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 5}, page_content='[13] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh\\nRao, Mu Wei, Naveen Valluri, Cliff Wong, Matthew P. Lungren, Tristan Naumann, and Hoifung\\nPoon. Large-scale domain-specific pretraining for biomedical vision-language processing, 2023.\\n[14] Michael Wornow, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming,\\nMichael A. Pfeffer, Jason Fries, and Nigam H. Shah. The shaky foundations of large lan-\\nguage models and foundation models for electronic health records. npj Digital Medicine ,\\n6(1):135, Jul 2023. ISSN 2398-6352. doi: 10.1038/s41746-023-00879-8. URL https:\\n//doi.org/10.1038/s41746-023-00879-8 .\\n6')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"55_JoLT_Jointly_Learned_Repres.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Please provide a summary of the following text:\n",
    "    Text: {text}\n",
    "    \"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"text\"], template=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mPlease provide a summary of the following text:\n",
      "    Text: JoLT: Jointly Learned Representations of\n",
      "Language and Time-Series\n",
      "Yifu Cai, Mononito Goswami, Arjun Choudhry, Arvind Srinivasan, Artur Dubrawski\n",
      "Auton Lab, School of Computer Science, Carnegie Mellon University\n",
      "Pittsburgh, PA 15213\n",
      "arvind.srini.8@gmail.com ,{yifuc, mgoswami, arjuncho, awd}@andrew.cmu.edu\n",
      "Abstract\n",
      "Time-series and text data is prevalent in healthcare and frequently exist in tandem,\n",
      "for e.g., in electrocardiogram (ECG) interpretation reports. Yet, these modalities\n",
      "are typically modeled independently. Even studies that jointly model time-series\n",
      "and text do so by converting time-series to images or graphs. We hypothesize\n",
      "that explicitly modeling time-series jointly with text can improve tasks such as\n",
      "summarization and question answering for time-series data, which have received\n",
      "little attention so far. To address this gap, we introduce JoLT to jointly learn desired\n",
      "representations from pre-trained time-series and text models. JoLT utilizes a\n",
      "Querying Transformer (Q-Former) to align the time-series and text representations.\n",
      "Our experiments on a large real-world electrocardiography dataset for medical time-\n",
      "series summarization show that JoLT outperforms state-of-the-art image captioning\n",
      "and medical question-answering approaches, and that the decoder architecture, size,\n",
      "and pre-training data can vary the performance on said tasks.\n",
      "1 Introduction\n",
      "Time-series and text data are frequently recorded in routine clinical care. But unlike general text\n",
      "or time-series, clinical data can only be analyzed by medical professionals, who spend substantial\n",
      "amounts of time analyzing biosignals, and entering summaries into electronic health records, away\n",
      "from direct patient care.\n",
      "To cater to an ever-increasing need to effectively and efficiently interpret clinical waveforms and text\n",
      "data, numerous studies have been devoted to automating clinical time-series and text interpretation.\n",
      "However, existing studies suffer from three key limitations. First, most existing studies model time-\n",
      "series and text independently, even when these modalities frequently co-exist, e.g., electrocardiogram\n",
      "(ECG) and clinical description of findings. Second, the few studies that jointly model time-series\n",
      "and text are primarily rule-based, and do not offer the fluency and versatility associated with neural\n",
      "approaches. Third, most existing multi-modal methods do not explicitly model time-series data,\n",
      "instead converting it to graphs or images and using graph or computer vision models, respectively.\n",
      "We introduce JoLT ,Jointly Learned Representations of Language and Time-series, a neural model\n",
      "which can generate text given time-series and textual prompts as input. We evaluate JoLT on a\n",
      "medical time-series summarization problem on the PTB-XL dataset, and compare it with a state-of-\n",
      "the-art image captioning model, BLIP-2 [ 1]. To the best of our knowledge, JoLT is one of the first\n",
      "automated ECG interpretation methods that explicitly models time-series to generate meaningful\n",
      "textual interpretations. Our experiments show that explicitly modeling time-series data can improve\n",
      "time-series summarization performance over state-of-the-art approaches pre-trained on vast amounts\n",
      "of data, and can enable tasks that can be obscure for time-series and text multi-modal data, like\n",
      "question answering.\n",
      "1st Workshop on Deep Generative Models for Health at NeurIPS 2023.\n",
      "\n",
      "Time-series\n",
      "EncoderQ-Former\n",
      "T ext...Language\n",
      "ModelGenerative Learning\n",
      "sinus rhythm type left\n",
      "bundle branch block\n",
      "Queries\n",
      "Representation\n",
      "LearningTime-series Pre-trainingFigure 1: An overview of JoLT . Given a time-series and an optional textual prompt as input, JoLT\n",
      "produces text as output. We pre-train a Transformer using the masked time-series reconstruction\n",
      "objective to use as an encoder, and the pre-trained language model as the decoder. The Q-Former\n",
      "is trained to align time-series and text representations. Learnable query tokens are used to extract\n",
      "time-series features conditioned on textual prompts.\n",
      "2 Related Work\n",
      "Time-series and Text Multimodal Models. Numerous studies have explored the problem of learning\n",
      "multimodal representations of data, such as graphs [ 2], image [ 1], and tabular data [ 3], grounded in\n",
      "text [ 4]. However, the challenge of jointly modeling time-series and text data has been relatively\n",
      "unexplored, primarily due to the lack of large publicly available paired time-series and text (pre-)\n",
      "training data, i.e., there is no equivalent of LAION-5B [ 5] or MS-COCO [ 6]. On the contrary, most\n",
      "existing time-series datasets are domain-specific, e.g. ECG interpretation [ 7] or stock price variations.\n",
      "This is exacerbated by the fact that most existing models are either statistical or rule-based, and\n",
      "necessitating substantial domain expertise that does not readily transfer across different domains [ 8].\n",
      "Clinical Text Summarization. In the healthcare domain, numerous studies have underscored the\n",
      "importance of developing automated text summarization systems. For instance, [ 9] highlights the\n",
      "pressing need for automated clinical report generation to alleviate the time burden on medical\n",
      "professionals, allowing them to focus more on patient care. Another relevant work by Harris and Zaki\n",
      "[10] introduced a CNN-LSTM framework designed to generate summaries of personal health data,\n",
      "such as heart rate, step count, and nutrient intake. Their approach demonstrated reasonably good\n",
      "performance in this context. However, it is worth noting that the quality of the generated summaries\n",
      "heavily relied on the paired training texts, which were created using rule-based methods. We expect\n",
      "neural methods to outperform neural methods relying on rule-based methods.\n",
      "3 Problem Formulation and Methods\n",
      "3.0.1 Time-series Summarization.\n",
      "Given a time-series T ∈RC×Lof length LwithCchannels, our goal is to generate a textual\n",
      "interpretation of salient time-series features in the context of a target domain.\n",
      "3.0.2 Model.\n",
      "JoLT comprises of a time-series encoder, a text decoder, and a transformer model which ties these\n",
      "two unimodal components together (Fig. 1) [ 11]. The time-series encoder is a transformer model\n",
      "which treats time-series sub-sequences as input tokens. Our best model uses the Open Pre-trained\n",
      "Trained (OPT) language model as a decoder, although we also evaluate the model with various other\n",
      "decoder models[ 12]. To align time-series and text representations, we leverage Querying Transformer\n",
      "(Q-Former) introduced by Li et al. [1].\n",
      "Pre-training Time-series Encoder. First, we break the input time-series into disjoint sub-sequences\n",
      "called patches. A small percentage of these patches are masked uniformly at random and then fed\n",
      "into the encoder, which is trained to reconstruct the masked patches using the Mean Squared Error\n",
      "loss.\n",
      "Representation Learning. In this stage, we freeze the pre-trained encoder and train the Q-Former\n",
      "to learn query embeddings that capture salient time-series representations that are informative of\n",
      "2\n",
      "\n",
      "sinus rhythm position type normal left bundle branch block left\n",
      "hypertrophy possible 4.46 unconfirmed reportGround\n",
      "Truth\n",
      "sinus rhythm. normal ecgFine-tuned\n",
      "BLIP-2\n",
      "sinus rhythm left type left bundle branch block left hypertrophy\n",
      "possible 4.46 unconfirmed reportJoLT\n",
      "(OPT-2.7B)\n",
      "Table 1: Qualitative evaluation of the results generated by JoLT compared to fine-tuned BLIP-2.\n",
      "JoLT (with OPT-2.7B decoder) generates text summaries very similar to the ground truth, while the\n",
      "fine-tuned BLIP-2 got the base class correct, but incorrectly described the time-series sample.\n",
      "input text. The Q-Former is trained using three objectives: (1) a contrastive loss to align time-series\n",
      "and text representations by maximizing their mutual information, (2) a text generation loss to train\n",
      "the Q-Former to generate text conditioned on input time-series, and a (3) time-series text matching\n",
      "lossfor finer grained alignment between time-series and text representations.\n",
      "Generative Learning. In this stage, we finally connect the frozen time-series encoder and Q-\n",
      "Former, with the frozen decoder, to leverage its generative capability. The query embeddings serve as\n",
      "soft prompts to guide the decoder’s language generation. We train the model end-to-end using the\n",
      "causal language modeling loss.\n",
      "4 Case Study: ECG Interpretation\n",
      "Dataset. We conduct an experiment on the PTB-XL dataset [ 7] to evaluate JoLT ’s ability to\n",
      "generate meaningful clinical interpretations from ECG waveform data. The dataset comprises of\n",
      "21,837 12-lead, 10 seconds long ECG recordings collected from 18,885 patients. A subset of ECG\n",
      "recordings is paired with gold-standard clinical interpretation, which we use to train and fine-tune\n",
      "our model. The train, validation, and test sets contain 11,319, 1,636, and 1,650 samples of paired\n",
      "time-series and text, respectively.\n",
      "Experimental Setup. We compare JoLT with the state-of-the-art image captioning model BLIP-2\n",
      "as baseline. We use the Matplotlib package1to transform time-series into graphical images before\n",
      "feeding them into BLIP-2 . We evaluate BLIP-2 in a zero-shot setting. Since the models are not\n",
      "pre-trained on medical data, we also compare JoLT with BLIP-2 fine-tuned on the PTB-XL dataset.\n",
      "We evaluate multiple metrics that are commonly used to evaluate text generation performance.\n",
      "We further run ablation experiments to evaluate the impact of the decoder on our model’s performance.\n",
      "Specifically, we evaluate different architectures (e.g. GPT-2-Large versus OPT), of different sizes\n",
      "(e.g. OPT-2.7B versus OPT-6.7B ), and pre-trained on different data (e.g. BioGPT-Large versus\n",
      "GPT-2-Large ). Beyond ECG interpretation, we also run preliminary experiments to explore our\n",
      "JoLT ’s ability to solve multiple choice questions conditioned on time-series, on the PTB-XL dataset.\n",
      "To the best of our knowledge, this unique but important problem has not been explored in prior work.\n",
      "We compare JoLT against BiomedCLIP [13] as a baseline.\n",
      "Tables 1, 2, 3, and 4 summarize the results of our experiments. Below, we highlight some key\n",
      "observations.\n",
      "Domain-specific fine-tuning is critical for clinical waveform interpretation. Poor performance\n",
      "of off-the-shelf BLIP-2 with respect to its fine-tuned counterpart shows the need for domain-specific\n",
      "fine-tuning, at least within the clinical domain. This motivates the need for publicly available large\n",
      "paired time-series and text datasets and models.\n",
      "1https://pypi.org/project/matplotlib/\n",
      "3\n",
      "\n",
      "Model Fine-tuned Rouge-1 Rouge-2 Rouge-LMETEOR BLEURTR P F 1 R P F 1 R P F 1\n",
      "BLIP-2 × 0.014 0.027 0.016 0.000 0.001 0.001 0.014 0.026 0.016 0.048 -1.283\n",
      "BLIP-2 ✓ 0.217 0.343 0.227 0.100 0.193 0.107 0.215 0.341 0.225 0.202 -0.930\n",
      "JoLT (OPT-2.7B) ✓ 0.404 0.528 0.436 0.277 0.355 0.295 0.403 0.526 0.435 0.414 -0.502\n",
      "Table 2: JoLT (with OPT-2.7B decoder) outperforms zero-shot and fine-tuned BLIP-2 baselines\n",
      "for the ECG interpretation task on the PTB-XL dataset. R,P, and F1denote the Recall, Precision,\n",
      "and F 1score.\n",
      "Decoder Rouge-1 Rouge-2 Rouge-LMETEOR BLEURTR P F 1 R P F 1 R P F 1\n",
      "OPT-2.7B 0.404 0.528 0.436 0.277 0.355 0.295 0.403 0.526 0.435 0.414 -0.502\n",
      "OPT-6.7B 0.400 0.518 0.429 0.277 0.350 0.294 0.399 0.517 0.428 0.408 -0.499\n",
      "GPT-2 0.113 0.420 0.169 0.020 0.080 0.029 0.113 0.419 0.168 0.017 -0.885\n",
      "BioGPT 0.107 0.342 0.153 0.022 0.072 0.031 0.107 0.342 0.153 0.212 -1.122\n",
      "BioMedLM 0.118 0.390 0.164 0.003 0.009 0.004 0.112 0.380 0.158 0.136 -1.079\n",
      "Table 3: JoLT with OPT-2.7B decoder outperforms JoLT with other decoders with different pre-\n",
      "trained data, different sizes, and different architectures on the ECG interpretation task on the PTB-XL\n",
      "dataset. R,P, and F1denote the Recall, Precision, and F 1score.\n",
      "Model Accuracy Weighted Precision Weighted Recall Weighted F1Score\n",
      "BiomedCLIP 0.11 0.57 0.11 0.02\n",
      "JoLT (OPT-2.7B) 0.54 0.31 0.54 0.4\n",
      "Table 4: Time-series conditioned multiple-choice question answering results on accuracy, precision,\n",
      "recall and F1-score. JoLT substantially outperforms BiomedCLIP , when given the prompt “ Which of\n",
      "the following five diagnostic classes does the following ECG belong to? \". Weighted averages are\n",
      "measured with respect to the support for each class.\n",
      "Explicitly modeling time-series improves summarization performance. JoLT produces textual\n",
      "summaries which are closer to ground truth compared to BLIP-2. We believe that the difference in\n",
      "performance largely stems from JoLT ’s ability to capture salient time-series features.\n",
      "Time-series and text joint modeling helps improve upon baselines for multi-class question\n",
      "answering. JoLT outperforms BiomedCLIP on time-series conditioned multiple choice question\n",
      "answering problem. However, we note that both the models perform poorly on this task. We believe\n",
      "that future work should carefully look to improve our model’s performance on this important task,\n",
      "with both models overconfidently predicting the majority class.\n",
      "Broken assumptions. For our ablation experiments, we hypothesized that: (1) Language models\n",
      "with more parameters will be better at text generation than smaller models, and (2) models pre-trained\n",
      "on clinical data ( BioGPT ,BiomedLM ) will be better than those trained on general text data ( GPT-2 ).\n",
      "However, our experiments did not support any of these hypothesis. The former can be partly explained\n",
      "by the fact that clinical interpretations are terse and do not require fluent large language models. We\n",
      "believe that further experiments are necessary to conclusively accept or reject the latter hypothesis.\n",
      "5 Conclusion and Future Work\n",
      "In this work, we introduced JoLT , a jointly model text and time-series data with a focus on ECG\n",
      "interpretation. We evaluated our model against state-of-the-art image captioning models in the context\n",
      "of clinical summarization. It’s worth noting a crucial aspect of our approach: the encoder of JoLT\n",
      "was pre-trained using a relatively small set of time-series data from the PTB-XL dataset. We posit\n",
      "that extending this pre-training phase to include a large amount of time-series data is likely to further\n",
      "improve its performance. Additionally, it’s important to acknowledge that the evaluation of these\n",
      "models presents significant challenges, as discussed in prior research [ 14]. Therefore, future research\n",
      "endeavors should aim to comprehensively and robustly assess the capabilities of such models in\n",
      "clinical applications.\n",
      "4\n",
      "\n",
      "6 Acknowledgments\n",
      "This work was partially supported by the U.S. Army Research Office and the U.S. Army Futures\n",
      "Command under contract W911NF-20-F-0020.\n",
      "References\n",
      "[1]Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image\n",
      "pre-training with frozen image encoders and large language models. In ICML , 2023.\n",
      "[2]Anthony Colas, Mehrdad Alvandipour, and Daisy Zhe Wang. GAP: A graph-aware lan-\n",
      "guage model framework for knowledge graph-to-text generation. In Proceedings of the 29th\n",
      "International Conference on Computational Linguistics , pages 5755–5769, Gyeongju, Repub-\n",
      "lic of Korea, October 2022. International Committee on Computational Linguistics. URL\n",
      "https://aclanthology.org/2022.coling-1.506 .\n",
      "[3]Rong Ye, Wenxian Shi, Hao Zhou, Zhongyu Wei, and Lei Li. Variational template machine for\n",
      "data-to-text generation. In International Conference on Learning Representations , 2020. URL\n",
      "https://openreview.net/forum?id=HkejNgBtPB .\n",
      "[4]Mandar Sharma, Ajay Gogineni, and Naren Ramakrishnan. Innovations in neural data-to-text\n",
      "generation: A survey, 2023.\n",
      "[5]Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\n",
      "Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n",
      "5b: An open large-scale dataset for training next generation image-text models. Advances in\n",
      "Neural Information Processing Systems , 35:25278–25294, 2022.\n",
      "[6]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\n",
      "Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\n",
      "Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\n",
      "Proceedings, Part V 13 , pages 740–755. Springer, 2014.\n",
      "[7]Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Dieter Kreiseler, Fatima I. Lunze,\n",
      "Wojciech Samek, and Tobias Schaeffter. Ptb-xl, a large publicly available electrocardiography\n",
      "dataset. Scientific Data , 2020. doi: https://doi.org/10.1038/s41597-020-0495-6.\n",
      "[8]Siddharth Biswal, Cao Xiao, M. Brandon Westover, and Jimeng Sun. Eegtotext: Learning\n",
      "to write medical reports from eeg recordings. In Finale Doshi-Velez, Jim Fackler, Ken Jung,\n",
      "David Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiens, editors, Proceedings of the\n",
      "4th Machine Learning for Healthcare Conference , volume 106 of Proceedings of Machine\n",
      "Learning Research , pages 513–531. PMLR, 09–10 Aug 2019. URL https://proceedings.\n",
      "mlr.press/v106/biswal19a.html .\n",
      "[9]Steffen Pauws, Albert Gatt, Emiel Krahmer, and Ehud Reiter. Making effective use of healthcare\n",
      "data using data-to-text technology, 2018.\n",
      "[10] Jonathan Harris and Mohammed J. Zaki. Towards neural numeric-to-text generation from\n",
      "temporal personal health data, 2022.\n",
      "[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
      "Ł ukasz Kaiser, and Illia Polosukhin. Transformers: Attention is all you need. In I. Guyon,\n",
      "U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, ed-\n",
      "itors, Advances in Neural Information Processing Systems , volume 30. Curran Associates,\n",
      "Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/\n",
      "3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .\n",
      "[12] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\n",
      "Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam\n",
      "Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke\n",
      "Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.\n",
      "5\n",
      "\n",
      "[13] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh\n",
      "Rao, Mu Wei, Naveen Valluri, Cliff Wong, Matthew P. Lungren, Tristan Naumann, and Hoifung\n",
      "Poon. Large-scale domain-specific pretraining for biomedical vision-language processing, 2023.\n",
      "[14] Michael Wornow, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming,\n",
      "Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. The shaky foundations of large lan-\n",
      "guage models and foundation models for electronic health records. npj Digital Medicine ,\n",
      "6(1):135, Jul 2023. ISSN 2398-6352. doi: 10.1038/s41746-023-00879-8. URL https:\n",
      "//doi.org/10.1038/s41746-023-00879-8 .\n",
      "6\n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The following is a summary of the provided text.\\n\\n**JoLT: Jointly Learned Representations of Language and Time-Series**\\n\\nThis paper introduces JoLT, a novel model for jointly learning representations of language and time-series data, with a specific focus on ECG interpretation.\\n\\n**Key aspects of JoLT:\\n\\n* **Time-series Encoder:** Trained using a masked time-series reconstruction objective.\\n* **Text Decoder:** Utilizes a pre-trained language model (e.g., OPT) to generate text summaries.\\n* **Querying Transformer (Q-Former) aligns time-series and text representations.\\n* **Fine-tuning: Both encoder and decoder are fine-tuned together.\\n\\n**Evaluations:**\\n\\n* **ECG interpretation task on the PTB-XL dataset. Compared to BLIP-2 (image captioning model).\\n* **Results:** JoLT outperforms BLIP-2 in ECG interpretation tasks.\\n* **Other language models tested: GPT-2, BioGPT, BioMedLM.\\n* **Findings: JoLT generally outperforms other models for ECG interpretation.\\n* **Time-series conditioned question answering: JoLT also outperforms BiomedCLIP on a time-series based question answering task.\\n\\n**Future work:**\\n\\n* **Larger time-series pre-training data.\\n* **Other evaluation metrics for healthcare application\\n\\n**\\n\\n**Limitations:**\\n\\n* **Evaluation challenges in healthcare data.\\n\\n**\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\n\\nThe paper presents JoLT, a novel model for jointly learning representations from language and time-series data, with a focus on ECG interpretation. \\n\\n**JoLT'import ant parts\\n\\n* **Time-series Encoder:\\n\\n**Text Decoder: Utilized a pre-trained language model. * **Querying Transformer: Aligns time-series and text representations.\\n* **Fine-tuning:\\n\\n**Results:\\n\\n**Future work:\\nLet me know if you'd like a more\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n**Limitations:\\n\\n* **Evaluation challenges in healthcare data.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n**Future work:\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n**Limitations:\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n This paper presents JoLT, a novel model for jointly learning representations from language and time-series data, with a focus on ECG interpretation.\\n\\n**JoLT:\\n\\n**Text Decoder: Utilized a pre-trained language model. * **Querying Transformer: Aligns time-series and text representations.\\n* **Fine-tuning:\\n\\n**Results:\\n\\n**Future work:\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n**Limitations:\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n**Evaluation challenges in healthcare data.\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n```python\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n**Limitations:\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n**Limitations:\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n**Limitations:\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n**Limitations:\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n**Limitations:\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n**Limitations:\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n**Limitations:\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n**Limitations:\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\nLet me know if you'd like a more detailed\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n**Limitations:\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n**Limitations:\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\nLet me know if you'd like a more detailed analysis of any specific aspect of the paper in general.\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\nLet me know if you'\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the pap\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\nLet me\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like a more\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\nLet me know if you'd like a more detailed analysis of any specific aspect of the text or the paper in general.\\n\\n\\n\\nLet me know if you'd like\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=prompt, verbose=True)\n",
    "summary = chain.run(pages)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map Reduce Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 0}, page_content='JoLT: Jointly Learned Representations of\\nLanguage and Time-Series\\nYifu Cai, Mononito Goswami, Arjun Choudhry, Arvind Srinivasan, Artur Dubrawski\\nAuton Lab, School of Computer Science, Carnegie Mellon University\\nPittsburgh, PA 15213\\narvind.srini.8@gmail.com ,{yifuc, mgoswami, arjuncho, awd}@andrew.cmu.edu\\nAbstract\\nTime-series and text data is prevalent in healthcare and frequently exist in tandem,\\nfor e.g., in electrocardiogram (ECG) interpretation reports. Yet, these modalities\\nare typically modeled independently. Even studies that jointly model time-series\\nand text do so by converting time-series to images or graphs. We hypothesize\\nthat explicitly modeling time-series jointly with text can improve tasks such as\\nsummarization and question answering for time-series data, which have received\\nlittle attention so far. To address this gap, we introduce JoLT to jointly learn desired\\nrepresentations from pre-trained time-series and text models. JoLT utilizes a\\nQuerying Transformer (Q-Former) to align the time-series and text representations.\\nOur experiments on a large real-world electrocardiography dataset for medical time-\\nseries summarization show that JoLT outperforms state-of-the-art image captioning\\nand medical question-answering approaches, and that the decoder architecture, size,\\nand pre-training data can vary the performance on said tasks.\\n1 Introduction\\nTime-series and text data are frequently recorded in routine clinical care. But unlike general text\\nor time-series, clinical data can only be analyzed by medical professionals, who spend substantial\\namounts of time analyzing biosignals, and entering summaries into electronic health records, away\\nfrom direct patient care.\\nTo cater to an ever-increasing need to effectively and efficiently interpret clinical waveforms and text\\ndata, numerous studies have been devoted to automating clinical time-series and text interpretation.\\nHowever, existing studies suffer from three key limitations. First, most existing studies model time-\\nseries and text independently, even when these modalities frequently co-exist, e.g., electrocardiogram\\n(ECG) and clinical description of findings. Second, the few studies that jointly model time-series\\nand text are primarily rule-based, and do not offer the fluency and versatility associated with neural\\napproaches. Third, most existing multi-modal methods do not explicitly model time-series data,\\ninstead converting it to graphs or images and using graph or computer vision models, respectively.\\nWe introduce JoLT ,Jointly Learned Representations of Language and Time-series, a neural model\\nwhich can generate text given time-series and textual prompts as input. We evaluate JoLT on a\\nmedical time-series summarization problem on the PTB-XL dataset, and compare it with a state-of-\\nthe-art image captioning model, BLIP-2 [ 1]. To the best of our knowledge, JoLT is one of the first\\nautomated ECG interpretation methods that explicitly models time-series to generate meaningful\\ntextual interpretations. Our experiments show that explicitly modeling time-series data can improve\\ntime-series summarization performance over state-of-the-art approaches pre-trained on vast amounts\\nof data, and can enable tasks that can be obscure for time-series and text multi-modal data, like\\nquestion answering.\\n1st Workshop on Deep Generative Models for Health at NeurIPS 2023.'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 1}, page_content='Time-series\\nEncoderQ-Former\\nT ext...Language\\nModelGenerative Learning\\nsinus rhythm type left\\nbundle branch block\\nQueries\\nRepresentation\\nLearningTime-series Pre-trainingFigure 1: An overview of JoLT . Given a time-series and an optional textual prompt as input, JoLT\\nproduces text as output. We pre-train a Transformer using the masked time-series reconstruction\\nobjective to use as an encoder, and the pre-trained language model as the decoder. The Q-Former\\nis trained to align time-series and text representations. Learnable query tokens are used to extract\\ntime-series features conditioned on textual prompts.\\n2 Related Work\\nTime-series and Text Multimodal Models. Numerous studies have explored the problem of learning\\nmultimodal representations of data, such as graphs [ 2], image [ 1], and tabular data [ 3], grounded in\\ntext [ 4]. However, the challenge of jointly modeling time-series and text data has been relatively\\nunexplored, primarily due to the lack of large publicly available paired time-series and text (pre-)\\ntraining data, i.e., there is no equivalent of LAION-5B [ 5] or MS-COCO [ 6]. On the contrary, most\\nexisting time-series datasets are domain-specific, e.g. ECG interpretation [ 7] or stock price variations.\\nThis is exacerbated by the fact that most existing models are either statistical or rule-based, and\\nnecessitating substantial domain expertise that does not readily transfer across different domains [ 8].\\nClinical Text Summarization. In the healthcare domain, numerous studies have underscored the\\nimportance of developing automated text summarization systems. For instance, [ 9] highlights the\\npressing need for automated clinical report generation to alleviate the time burden on medical\\nprofessionals, allowing them to focus more on patient care. Another relevant work by Harris and Zaki\\n[10] introduced a CNN-LSTM framework designed to generate summaries of personal health data,\\nsuch as heart rate, step count, and nutrient intake. Their approach demonstrated reasonably good\\nperformance in this context. However, it is worth noting that the quality of the generated summaries\\nheavily relied on the paired training texts, which were created using rule-based methods. We expect\\nneural methods to outperform neural methods relying on rule-based methods.\\n3 Problem Formulation and Methods\\n3.0.1 Time-series Summarization.\\nGiven a time-series T ∈RC×Lof length LwithCchannels, our goal is to generate a textual\\ninterpretation of salient time-series features in the context of a target domain.\\n3.0.2 Model.\\nJoLT comprises of a time-series encoder, a text decoder, and a transformer model which ties these\\ntwo unimodal components together (Fig. 1) [ 11]. The time-series encoder is a transformer model\\nwhich treats time-series sub-sequences as input tokens. Our best model uses the Open Pre-trained\\nTrained (OPT) language model as a decoder, although we also evaluate the model with various other\\ndecoder models[ 12]. To align time-series and text representations, we leverage Querying Transformer\\n(Q-Former) introduced by Li et al. [1].\\nPre-training Time-series Encoder. First, we break the input time-series into disjoint sub-sequences\\ncalled patches. A small percentage of these patches are masked uniformly at random and then fed\\ninto the encoder, which is trained to reconstruct the masked patches using the Mean Squared Error\\nloss.\\nRepresentation Learning. In this stage, we freeze the pre-trained encoder and train the Q-Former\\nto learn query embeddings that capture salient time-series representations that are informative of\\n2'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 2}, page_content='sinus rhythm position type normal left bundle branch block left\\nhypertrophy possible 4.46 unconfirmed reportGround\\nTruth\\nsinus rhythm. normal ecgFine-tuned\\nBLIP-2\\nsinus rhythm left type left bundle branch block left hypertrophy\\npossible 4.46 unconfirmed reportJoLT\\n(OPT-2.7B)\\nTable 1: Qualitative evaluation of the results generated by JoLT compared to fine-tuned BLIP-2.\\nJoLT (with OPT-2.7B decoder) generates text summaries very similar to the ground truth, while the\\nfine-tuned BLIP-2 got the base class correct, but incorrectly described the time-series sample.\\ninput text. The Q-Former is trained using three objectives: (1) a contrastive loss to align time-series\\nand text representations by maximizing their mutual information, (2) a text generation loss to train\\nthe Q-Former to generate text conditioned on input time-series, and a (3) time-series text matching\\nlossfor finer grained alignment between time-series and text representations.\\nGenerative Learning. In this stage, we finally connect the frozen time-series encoder and Q-\\nFormer, with the frozen decoder, to leverage its generative capability. The query embeddings serve as\\nsoft prompts to guide the decoder’s language generation. We train the model end-to-end using the\\ncausal language modeling loss.\\n4 Case Study: ECG Interpretation\\nDataset. We conduct an experiment on the PTB-XL dataset [ 7] to evaluate JoLT ’s ability to\\ngenerate meaningful clinical interpretations from ECG waveform data. The dataset comprises of\\n21,837 12-lead, 10 seconds long ECG recordings collected from 18,885 patients. A subset of ECG\\nrecordings is paired with gold-standard clinical interpretation, which we use to train and fine-tune\\nour model. The train, validation, and test sets contain 11,319, 1,636, and 1,650 samples of paired\\ntime-series and text, respectively.\\nExperimental Setup. We compare JoLT with the state-of-the-art image captioning model BLIP-2\\nas baseline. We use the Matplotlib package1to transform time-series into graphical images before\\nfeeding them into BLIP-2 . We evaluate BLIP-2 in a zero-shot setting. Since the models are not\\npre-trained on medical data, we also compare JoLT with BLIP-2 fine-tuned on the PTB-XL dataset.\\nWe evaluate multiple metrics that are commonly used to evaluate text generation performance.\\nWe further run ablation experiments to evaluate the impact of the decoder on our model’s performance.\\nSpecifically, we evaluate different architectures (e.g. GPT-2-Large versus OPT), of different sizes\\n(e.g. OPT-2.7B versus OPT-6.7B ), and pre-trained on different data (e.g. BioGPT-Large versus\\nGPT-2-Large ). Beyond ECG interpretation, we also run preliminary experiments to explore our\\nJoLT ’s ability to solve multiple choice questions conditioned on time-series, on the PTB-XL dataset.\\nTo the best of our knowledge, this unique but important problem has not been explored in prior work.\\nWe compare JoLT against BiomedCLIP [13] as a baseline.\\nTables 1, 2, 3, and 4 summarize the results of our experiments. Below, we highlight some key\\nobservations.\\nDomain-specific fine-tuning is critical for clinical waveform interpretation. Poor performance\\nof off-the-shelf BLIP-2 with respect to its fine-tuned counterpart shows the need for domain-specific\\nfine-tuning, at least within the clinical domain. This motivates the need for publicly available large\\npaired time-series and text datasets and models.\\n1https://pypi.org/project/matplotlib/\\n3'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 3}, page_content='Model Fine-tuned Rouge-1 Rouge-2 Rouge-LMETEOR BLEURTR P F 1 R P F 1 R P F 1\\nBLIP-2 × 0.014 0.027 0.016 0.000 0.001 0.001 0.014 0.026 0.016 0.048 -1.283\\nBLIP-2 ✓ 0.217 0.343 0.227 0.100 0.193 0.107 0.215 0.341 0.225 0.202 -0.930\\nJoLT (OPT-2.7B) ✓ 0.404 0.528 0.436 0.277 0.355 0.295 0.403 0.526 0.435 0.414 -0.502\\nTable 2: JoLT (with OPT-2.7B decoder) outperforms zero-shot and fine-tuned BLIP-2 baselines\\nfor the ECG interpretation task on the PTB-XL dataset. R,P, and F1denote the Recall, Precision,\\nand F 1score.\\nDecoder Rouge-1 Rouge-2 Rouge-LMETEOR BLEURTR P F 1 R P F 1 R P F 1\\nOPT-2.7B 0.404 0.528 0.436 0.277 0.355 0.295 0.403 0.526 0.435 0.414 -0.502\\nOPT-6.7B 0.400 0.518 0.429 0.277 0.350 0.294 0.399 0.517 0.428 0.408 -0.499\\nGPT-2 0.113 0.420 0.169 0.020 0.080 0.029 0.113 0.419 0.168 0.017 -0.885\\nBioGPT 0.107 0.342 0.153 0.022 0.072 0.031 0.107 0.342 0.153 0.212 -1.122\\nBioMedLM 0.118 0.390 0.164 0.003 0.009 0.004 0.112 0.380 0.158 0.136 -1.079\\nTable 3: JoLT with OPT-2.7B decoder outperforms JoLT with other decoders with different pre-\\ntrained data, different sizes, and different architectures on the ECG interpretation task on the PTB-XL\\ndataset. R,P, and F1denote the Recall, Precision, and F 1score.\\nModel Accuracy Weighted Precision Weighted Recall Weighted F1Score\\nBiomedCLIP 0.11 0.57 0.11 0.02\\nJoLT (OPT-2.7B) 0.54 0.31 0.54 0.4\\nTable 4: Time-series conditioned multiple-choice question answering results on accuracy, precision,\\nrecall and F1-score. JoLT substantially outperforms BiomedCLIP , when given the prompt “ Which of\\nthe following five diagnostic classes does the following ECG belong to? \". Weighted averages are\\nmeasured with respect to the support for each class.\\nExplicitly modeling time-series improves summarization performance. JoLT produces textual\\nsummaries which are closer to ground truth compared to BLIP-2. We believe that the difference in\\nperformance largely stems from JoLT ’s ability to capture salient time-series features.\\nTime-series and text joint modeling helps improve upon baselines for multi-class question\\nanswering. JoLT outperforms BiomedCLIP on time-series conditioned multiple choice question\\nanswering problem. However, we note that both the models perform poorly on this task. We believe\\nthat future work should carefully look to improve our model’s performance on this important task,\\nwith both models overconfidently predicting the majority class.\\nBroken assumptions. For our ablation experiments, we hypothesized that: (1) Language models\\nwith more parameters will be better at text generation than smaller models, and (2) models pre-trained\\non clinical data ( BioGPT ,BiomedLM ) will be better than those trained on general text data ( GPT-2 ).\\nHowever, our experiments did not support any of these hypothesis. The former can be partly explained\\nby the fact that clinical interpretations are terse and do not require fluent large language models. We\\nbelieve that further experiments are necessary to conclusively accept or reject the latter hypothesis.\\n5 Conclusion and Future Work\\nIn this work, we introduced JoLT , a jointly model text and time-series data with a focus on ECG\\ninterpretation. We evaluated our model against state-of-the-art image captioning models in the context\\nof clinical summarization. It’s worth noting a crucial aspect of our approach: the encoder of JoLT\\nwas pre-trained using a relatively small set of time-series data from the PTB-XL dataset. We posit\\nthat extending this pre-training phase to include a large amount of time-series data is likely to further\\nimprove its performance. Additionally, it’s important to acknowledge that the evaluation of these\\nmodels presents significant challenges, as discussed in prior research [ 14]. Therefore, future research\\nendeavors should aim to comprehensively and robustly assess the capabilities of such models in\\nclinical applications.\\n4'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 4}, page_content='6 Acknowledgments\\nThis work was partially supported by the U.S. Army Research Office and the U.S. Army Futures\\nCommand under contract W911NF-20-F-0020.\\nReferences\\n[1]Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image\\npre-training with frozen image encoders and large language models. In ICML , 2023.\\n[2]Anthony Colas, Mehrdad Alvandipour, and Daisy Zhe Wang. GAP: A graph-aware lan-\\nguage model framework for knowledge graph-to-text generation. In Proceedings of the 29th\\nInternational Conference on Computational Linguistics , pages 5755–5769, Gyeongju, Repub-\\nlic of Korea, October 2022. International Committee on Computational Linguistics. URL\\nhttps://aclanthology.org/2022.coling-1.506 .\\n[3]Rong Ye, Wenxian Shi, Hao Zhou, Zhongyu Wei, and Lei Li. Variational template machine for\\ndata-to-text generation. In International Conference on Learning Representations , 2020. URL\\nhttps://openreview.net/forum?id=HkejNgBtPB .\\n[4]Mandar Sharma, Ajay Gogineni, and Naren Ramakrishnan. Innovations in neural data-to-text\\ngeneration: A survey, 2023.\\n[5]Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\\n5b: An open large-scale dataset for training next generation image-text models. Advances in\\nNeural Information Processing Systems , 35:25278–25294, 2022.\\n[6]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\\nVision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\\nProceedings, Part V 13 , pages 740–755. Springer, 2014.\\n[7]Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Dieter Kreiseler, Fatima I. Lunze,\\nWojciech Samek, and Tobias Schaeffter. Ptb-xl, a large publicly available electrocardiography\\ndataset. Scientific Data , 2020. doi: https://doi.org/10.1038/s41597-020-0495-6.\\n[8]Siddharth Biswal, Cao Xiao, M. Brandon Westover, and Jimeng Sun. Eegtotext: Learning\\nto write medical reports from eeg recordings. In Finale Doshi-Velez, Jim Fackler, Ken Jung,\\nDavid Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiens, editors, Proceedings of the\\n4th Machine Learning for Healthcare Conference , volume 106 of Proceedings of Machine\\nLearning Research , pages 513–531. PMLR, 09–10 Aug 2019. URL https://proceedings.\\nmlr.press/v106/biswal19a.html .\\n[9]Steffen Pauws, Albert Gatt, Emiel Krahmer, and Ehud Reiter. Making effective use of healthcare\\ndata using data-to-text technology, 2018.\\n[10] Jonathan Harris and Mohammed J. Zaki. Towards neural numeric-to-text generation from\\ntemporal personal health data, 2022.\\n[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁ ukasz Kaiser, and Illia Polosukhin. Transformers: Attention is all you need. In I. Guyon,\\nU. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, ed-\\nitors, Advances in Neural Information Processing Systems , volume 30. Curran Associates,\\nInc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/\\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .\\n[12] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam\\nShleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke\\nZettlemoyer. Opt: Open pre-trained transformer language models, 2022.\\n5'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 5}, page_content='[13] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh\\nRao, Mu Wei, Naveen Valluri, Cliff Wong, Matthew P. Lungren, Tristan Naumann, and Hoifung\\nPoon. Large-scale domain-specific pretraining for biomedical vision-language processing, 2023.\\n[14] Michael Wornow, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming,\\nMichael A. Pfeffer, Jason Fries, and Nigam H. Shah. The shaky foundations of large lan-\\nguage models and foundation models for electronic health records. npj Digital Medicine ,\\n6(1):135, Jul 2023. ISSN 2398-6352. doi: 10.1038/s41746-023-00879-8. URL https:\\n//doi.org/10.1038/s41746-023-00879-8 .\\n6')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"55_JoLT_Jointly_Learned_Repres.pdf\")\n",
    "docs = loader.load_and_split()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 0}, page_content='JoLT: Jointly Learned Representations of\\nLanguage and Time-Series\\nYifu Cai, Mononito Goswami, Arjun Choudhry, Arvind Srinivasan, Artur Dubrawski\\nAuton Lab, School of Computer Science, Carnegie Mellon University\\nPittsburgh, PA 15213\\narvind.srini.8@gmail.com ,{yifuc, mgoswami, arjuncho, awd}@andrew.cmu.edu\\nAbstract\\nTime-series and text data is prevalent in healthcare and frequently exist in tandem,\\nfor e.g., in electrocardiogram (ECG) interpretation reports. Yet, these modalities\\nare typically modeled independently. Even studies that jointly model time-series\\nand text do so by converting time-series to images or graphs. We hypothesize\\nthat explicitly modeling time-series jointly with text can improve tasks such as\\nsummarization and question answering for time-series data, which have received\\nlittle attention so far. To address this gap, we introduce JoLT to jointly learn desired\\nrepresentations from pre-trained time-series and text models. JoLT utilizes a\\nQuerying Transformer (Q-Former) to align the time-series and text representations.\\nOur experiments on a large real-world electrocardiography dataset for medical time-\\nseries summarization show that JoLT outperforms state-of-the-art image captioning\\nand medical question-answering approaches, and that the decoder architecture, size,\\nand pre-training data can vary the performance on said tasks.\\n1 Introduction\\nTime-series and text data are frequently recorded in routine clinical care. But unlike general text\\nor time-series, clinical data can only be analyzed by medical professionals, who spend substantial\\namounts of time analyzing biosignals, and entering summaries into electronic health records, away\\nfrom direct patient care.\\nTo cater to an ever-increasing need to effectively and efficiently interpret clinical waveforms and text\\ndata, numerous studies have been devoted to automating clinical time-series and text interpretation.\\nHowever, existing studies suffer from three key limitations. First, most existing studies model time-'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 0}, page_content='However, existing studies suffer from three key limitations. First, most existing studies model time-\\nseries and text independently, even when these modalities frequently co-exist, e.g., electrocardiogram\\n(ECG) and clinical description of findings. Second, the few studies that jointly model time-series\\nand text are primarily rule-based, and do not offer the fluency and versatility associated with neural\\napproaches. Third, most existing multi-modal methods do not explicitly model time-series data,\\ninstead converting it to graphs or images and using graph or computer vision models, respectively.\\nWe introduce JoLT ,Jointly Learned Representations of Language and Time-series, a neural model\\nwhich can generate text given time-series and textual prompts as input. We evaluate JoLT on a\\nmedical time-series summarization problem on the PTB-XL dataset, and compare it with a state-of-\\nthe-art image captioning model, BLIP-2 [ 1]. To the best of our knowledge, JoLT is one of the first\\nautomated ECG interpretation methods that explicitly models time-series to generate meaningful\\ntextual interpretations. Our experiments show that explicitly modeling time-series data can improve\\ntime-series summarization performance over state-of-the-art approaches pre-trained on vast amounts\\nof data, and can enable tasks that can be obscure for time-series and text multi-modal data, like\\nquestion answering.\\n1st Workshop on Deep Generative Models for Health at NeurIPS 2023.'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 1}, page_content='Time-series\\nEncoderQ-Former\\nT ext...Language\\nModelGenerative Learning\\nsinus rhythm type left\\nbundle branch block\\nQueries\\nRepresentation\\nLearningTime-series Pre-trainingFigure 1: An overview of JoLT . Given a time-series and an optional textual prompt as input, JoLT\\nproduces text as output. We pre-train a Transformer using the masked time-series reconstruction\\nobjective to use as an encoder, and the pre-trained language model as the decoder. The Q-Former\\nis trained to align time-series and text representations. Learnable query tokens are used to extract\\ntime-series features conditioned on textual prompts.\\n2 Related Work\\nTime-series and Text Multimodal Models. Numerous studies have explored the problem of learning\\nmultimodal representations of data, such as graphs [ 2], image [ 1], and tabular data [ 3], grounded in\\ntext [ 4]. However, the challenge of jointly modeling time-series and text data has been relatively\\nunexplored, primarily due to the lack of large publicly available paired time-series and text (pre-)\\ntraining data, i.e., there is no equivalent of LAION-5B [ 5] or MS-COCO [ 6]. On the contrary, most\\nexisting time-series datasets are domain-specific, e.g. ECG interpretation [ 7] or stock price variations.\\nThis is exacerbated by the fact that most existing models are either statistical or rule-based, and\\nnecessitating substantial domain expertise that does not readily transfer across different domains [ 8].\\nClinical Text Summarization. In the healthcare domain, numerous studies have underscored the\\nimportance of developing automated text summarization systems. For instance, [ 9] highlights the\\npressing need for automated clinical report generation to alleviate the time burden on medical\\nprofessionals, allowing them to focus more on patient care. Another relevant work by Harris and Zaki\\n[10] introduced a CNN-LSTM framework designed to generate summaries of personal health data,\\nsuch as heart rate, step count, and nutrient intake. Their approach demonstrated reasonably good'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 1}, page_content='such as heart rate, step count, and nutrient intake. Their approach demonstrated reasonably good\\nperformance in this context. However, it is worth noting that the quality of the generated summaries\\nheavily relied on the paired training texts, which were created using rule-based methods. We expect\\nneural methods to outperform neural methods relying on rule-based methods.\\n3 Problem Formulation and Methods\\n3.0.1 Time-series Summarization.\\nGiven a time-series T ∈RC×Lof length LwithCchannels, our goal is to generate a textual\\ninterpretation of salient time-series features in the context of a target domain.\\n3.0.2 Model.\\nJoLT comprises of a time-series encoder, a text decoder, and a transformer model which ties these\\ntwo unimodal components together (Fig. 1) [ 11]. The time-series encoder is a transformer model\\nwhich treats time-series sub-sequences as input tokens. Our best model uses the Open Pre-trained\\nTrained (OPT) language model as a decoder, although we also evaluate the model with various other\\ndecoder models[ 12]. To align time-series and text representations, we leverage Querying Transformer\\n(Q-Former) introduced by Li et al. [1].\\nPre-training Time-series Encoder. First, we break the input time-series into disjoint sub-sequences\\ncalled patches. A small percentage of these patches are masked uniformly at random and then fed\\ninto the encoder, which is trained to reconstruct the masked patches using the Mean Squared Error\\nloss.\\nRepresentation Learning. In this stage, we freeze the pre-trained encoder and train the Q-Former\\nto learn query embeddings that capture salient time-series representations that are informative of\\n2'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 2}, page_content='sinus rhythm position type normal left bundle branch block left\\nhypertrophy possible 4.46 unconfirmed reportGround\\nTruth\\nsinus rhythm. normal ecgFine-tuned\\nBLIP-2\\nsinus rhythm left type left bundle branch block left hypertrophy\\npossible 4.46 unconfirmed reportJoLT\\n(OPT-2.7B)\\nTable 1: Qualitative evaluation of the results generated by JoLT compared to fine-tuned BLIP-2.\\nJoLT (with OPT-2.7B decoder) generates text summaries very similar to the ground truth, while the\\nfine-tuned BLIP-2 got the base class correct, but incorrectly described the time-series sample.\\ninput text. The Q-Former is trained using three objectives: (1) a contrastive loss to align time-series\\nand text representations by maximizing their mutual information, (2) a text generation loss to train\\nthe Q-Former to generate text conditioned on input time-series, and a (3) time-series text matching\\nlossfor finer grained alignment between time-series and text representations.\\nGenerative Learning. In this stage, we finally connect the frozen time-series encoder and Q-\\nFormer, with the frozen decoder, to leverage its generative capability. The query embeddings serve as\\nsoft prompts to guide the decoder’s language generation. We train the model end-to-end using the\\ncausal language modeling loss.\\n4 Case Study: ECG Interpretation\\nDataset. We conduct an experiment on the PTB-XL dataset [ 7] to evaluate JoLT ’s ability to\\ngenerate meaningful clinical interpretations from ECG waveform data. The dataset comprises of\\n21,837 12-lead, 10 seconds long ECG recordings collected from 18,885 patients. A subset of ECG\\nrecordings is paired with gold-standard clinical interpretation, which we use to train and fine-tune\\nour model. The train, validation, and test sets contain 11,319, 1,636, and 1,650 samples of paired\\ntime-series and text, respectively.\\nExperimental Setup. We compare JoLT with the state-of-the-art image captioning model BLIP-2\\nas baseline. We use the Matplotlib package1to transform time-series into graphical images before'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 2}, page_content='as baseline. We use the Matplotlib package1to transform time-series into graphical images before\\nfeeding them into BLIP-2 . We evaluate BLIP-2 in a zero-shot setting. Since the models are not\\npre-trained on medical data, we also compare JoLT with BLIP-2 fine-tuned on the PTB-XL dataset.\\nWe evaluate multiple metrics that are commonly used to evaluate text generation performance.\\nWe further run ablation experiments to evaluate the impact of the decoder on our model’s performance.\\nSpecifically, we evaluate different architectures (e.g. GPT-2-Large versus OPT), of different sizes\\n(e.g. OPT-2.7B versus OPT-6.7B ), and pre-trained on different data (e.g. BioGPT-Large versus\\nGPT-2-Large ). Beyond ECG interpretation, we also run preliminary experiments to explore our\\nJoLT ’s ability to solve multiple choice questions conditioned on time-series, on the PTB-XL dataset.\\nTo the best of our knowledge, this unique but important problem has not been explored in prior work.\\nWe compare JoLT against BiomedCLIP [13] as a baseline.\\nTables 1, 2, 3, and 4 summarize the results of our experiments. Below, we highlight some key\\nobservations.\\nDomain-specific fine-tuning is critical for clinical waveform interpretation. Poor performance\\nof off-the-shelf BLIP-2 with respect to its fine-tuned counterpart shows the need for domain-specific\\nfine-tuning, at least within the clinical domain. This motivates the need for publicly available large\\npaired time-series and text datasets and models.\\n1https://pypi.org/project/matplotlib/\\n3'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 3}, page_content='Model Fine-tuned Rouge-1 Rouge-2 Rouge-LMETEOR BLEURTR P F 1 R P F 1 R P F 1\\nBLIP-2 × 0.014 0.027 0.016 0.000 0.001 0.001 0.014 0.026 0.016 0.048 -1.283\\nBLIP-2 ✓ 0.217 0.343 0.227 0.100 0.193 0.107 0.215 0.341 0.225 0.202 -0.930\\nJoLT (OPT-2.7B) ✓ 0.404 0.528 0.436 0.277 0.355 0.295 0.403 0.526 0.435 0.414 -0.502\\nTable 2: JoLT (with OPT-2.7B decoder) outperforms zero-shot and fine-tuned BLIP-2 baselines\\nfor the ECG interpretation task on the PTB-XL dataset. R,P, and F1denote the Recall, Precision,\\nand F 1score.\\nDecoder Rouge-1 Rouge-2 Rouge-LMETEOR BLEURTR P F 1 R P F 1 R P F 1\\nOPT-2.7B 0.404 0.528 0.436 0.277 0.355 0.295 0.403 0.526 0.435 0.414 -0.502\\nOPT-6.7B 0.400 0.518 0.429 0.277 0.350 0.294 0.399 0.517 0.428 0.408 -0.499\\nGPT-2 0.113 0.420 0.169 0.020 0.080 0.029 0.113 0.419 0.168 0.017 -0.885\\nBioGPT 0.107 0.342 0.153 0.022 0.072 0.031 0.107 0.342 0.153 0.212 -1.122\\nBioMedLM 0.118 0.390 0.164 0.003 0.009 0.004 0.112 0.380 0.158 0.136 -1.079\\nTable 3: JoLT with OPT-2.7B decoder outperforms JoLT with other decoders with different pre-\\ntrained data, different sizes, and different architectures on the ECG interpretation task on the PTB-XL\\ndataset. R,P, and F1denote the Recall, Precision, and F 1score.\\nModel Accuracy Weighted Precision Weighted Recall Weighted F1Score\\nBiomedCLIP 0.11 0.57 0.11 0.02\\nJoLT (OPT-2.7B) 0.54 0.31 0.54 0.4\\nTable 4: Time-series conditioned multiple-choice question answering results on accuracy, precision,\\nrecall and F1-score. JoLT substantially outperforms BiomedCLIP , when given the prompt “ Which of\\nthe following five diagnostic classes does the following ECG belong to? \". Weighted averages are\\nmeasured with respect to the support for each class.\\nExplicitly modeling time-series improves summarization performance. JoLT produces textual\\nsummaries which are closer to ground truth compared to BLIP-2. We believe that the difference in\\nperformance largely stems from JoLT ’s ability to capture salient time-series features.'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 3}, page_content='performance largely stems from JoLT ’s ability to capture salient time-series features.\\nTime-series and text joint modeling helps improve upon baselines for multi-class question\\nanswering. JoLT outperforms BiomedCLIP on time-series conditioned multiple choice question\\nanswering problem. However, we note that both the models perform poorly on this task. We believe\\nthat future work should carefully look to improve our model’s performance on this important task,\\nwith both models overconfidently predicting the majority class.\\nBroken assumptions. For our ablation experiments, we hypothesized that: (1) Language models\\nwith more parameters will be better at text generation than smaller models, and (2) models pre-trained\\non clinical data ( BioGPT ,BiomedLM ) will be better than those trained on general text data ( GPT-2 ).\\nHowever, our experiments did not support any of these hypothesis. The former can be partly explained\\nby the fact that clinical interpretations are terse and do not require fluent large language models. We\\nbelieve that further experiments are necessary to conclusively accept or reject the latter hypothesis.\\n5 Conclusion and Future Work\\nIn this work, we introduced JoLT , a jointly model text and time-series data with a focus on ECG\\ninterpretation. We evaluated our model against state-of-the-art image captioning models in the context\\nof clinical summarization. It’s worth noting a crucial aspect of our approach: the encoder of JoLT\\nwas pre-trained using a relatively small set of time-series data from the PTB-XL dataset. We posit\\nthat extending this pre-training phase to include a large amount of time-series data is likely to further\\nimprove its performance. Additionally, it’s important to acknowledge that the evaluation of these\\nmodels presents significant challenges, as discussed in prior research [ 14]. Therefore, future research\\nendeavors should aim to comprehensively and robustly assess the capabilities of such models in\\nclinical applications.\\n4'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 4}, page_content='6 Acknowledgments\\nThis work was partially supported by the U.S. Army Research Office and the U.S. Army Futures\\nCommand under contract W911NF-20-F-0020.\\nReferences\\n[1]Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image\\npre-training with frozen image encoders and large language models. In ICML , 2023.\\n[2]Anthony Colas, Mehrdad Alvandipour, and Daisy Zhe Wang. GAP: A graph-aware lan-\\nguage model framework for knowledge graph-to-text generation. In Proceedings of the 29th\\nInternational Conference on Computational Linguistics , pages 5755–5769, Gyeongju, Repub-\\nlic of Korea, October 2022. International Committee on Computational Linguistics. URL\\nhttps://aclanthology.org/2022.coling-1.506 .\\n[3]Rong Ye, Wenxian Shi, Hao Zhou, Zhongyu Wei, and Lei Li. Variational template machine for\\ndata-to-text generation. In International Conference on Learning Representations , 2020. URL\\nhttps://openreview.net/forum?id=HkejNgBtPB .\\n[4]Mandar Sharma, Ajay Gogineni, and Naren Ramakrishnan. Innovations in neural data-to-text\\ngeneration: A survey, 2023.\\n[5]Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\\n5b: An open large-scale dataset for training next generation image-text models. Advances in\\nNeural Information Processing Systems , 35:25278–25294, 2022.\\n[6]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\\nDollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\\nVision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\\nProceedings, Part V 13 , pages 740–755. Springer, 2014.\\n[7]Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Dieter Kreiseler, Fatima I. Lunze,\\nWojciech Samek, and Tobias Schaeffter. Ptb-xl, a large publicly available electrocardiography\\ndataset. Scientific Data , 2020. doi: https://doi.org/10.1038/s41597-020-0495-6.'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 4}, page_content='dataset. Scientific Data , 2020. doi: https://doi.org/10.1038/s41597-020-0495-6.\\n[8]Siddharth Biswal, Cao Xiao, M. Brandon Westover, and Jimeng Sun. Eegtotext: Learning\\nto write medical reports from eeg recordings. In Finale Doshi-Velez, Jim Fackler, Ken Jung,\\nDavid Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiens, editors, Proceedings of the\\n4th Machine Learning for Healthcare Conference , volume 106 of Proceedings of Machine\\nLearning Research , pages 513–531. PMLR, 09–10 Aug 2019. URL https://proceedings.\\nmlr.press/v106/biswal19a.html .\\n[9]Steffen Pauws, Albert Gatt, Emiel Krahmer, and Ehud Reiter. Making effective use of healthcare\\ndata using data-to-text technology, 2018.\\n[10] Jonathan Harris and Mohammed J. Zaki. Towards neural numeric-to-text generation from\\ntemporal personal health data, 2022.\\n[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁ ukasz Kaiser, and Illia Polosukhin. Transformers: Attention is all you need. In I. Guyon,\\nU. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, ed-\\nitors, Advances in Neural Information Processing Systems , volume 30. Curran Associates,\\nInc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/\\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .\\n[12] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam\\nShleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke\\nZettlemoyer. Opt: Open pre-trained transformer language models, 2022.\\n5'),\n",
       " Document(metadata={'source': '55_JoLT_Jointly_Learned_Repres.pdf', 'page': 5}, page_content='[13] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh\\nRao, Mu Wei, Naveen Valluri, Cliff Wong, Matthew P. Lungren, Tristan Naumann, and Hoifung\\nPoon. Large-scale domain-specific pretraining for biomedical vision-language processing, 2023.\\n[14] Michael Wornow, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming,\\nMichael A. Pfeffer, Jason Fries, and Nigam H. Shah. The shaky foundations of large lan-\\nguage models and foundation models for electronic health records. npj Digital Medicine ,\\n6(1):135, Jul 2023. ISSN 2398-6352. doi: 10.1038/s41746-023-00879-8. URL https:\\n//doi.org/10.1038/s41746-023-00879-8 .\\n6')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_docs = RecursiveCharacterTextSplitter(chunk_size = 2048, chunk_overlap = 128).split_documents(docs)\n",
    "splitted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_prompt = \"\"\"Please provide a summary of the following article:\n",
    "    Article: {text}\n",
    "    \"\"\"\n",
    "chunk_prompt = PromptTemplate(input_variables=[\"text\"], template=chunk_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = \"\"\"\n",
    "    Provide the final summary of the entire article with the important points.\n",
    "    Add a Motivation Title,Start the precise summary with an introduction and provide the summary in number \n",
    "    points for the article.\n",
    "    Article:{text}\"\"\"\n",
    "final_prompt = PromptTemplate(input_variables=[\"text\"], template=final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain = load_summarize_chain(llm, chain_type=\"map_reduce\", map_prompt=chunk_prompt, combine_prompt = final_prompt,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mPlease provide a summary of the following article:\n",
      "    Article: JoLT: Jointly Learned Representations of\n",
      "Language and Time-Series\n",
      "Yifu Cai, Mononito Goswami, Arjun Choudhry, Arvind Srinivasan, Artur Dubrawski\n",
      "Auton Lab, School of Computer Science, Carnegie Mellon University\n",
      "Pittsburgh, PA 15213\n",
      "arvind.srini.8@gmail.com ,{yifuc, mgoswami, arjuncho, awd}@andrew.cmu.edu\n",
      "Abstract\n",
      "Time-series and text data is prevalent in healthcare and frequently exist in tandem,\n",
      "for e.g., in electrocardiogram (ECG) interpretation reports. Yet, these modalities\n",
      "are typically modeled independently. Even studies that jointly model time-series\n",
      "and text do so by converting time-series to images or graphs. We hypothesize\n",
      "that explicitly modeling time-series jointly with text can improve tasks such as\n",
      "summarization and question answering for time-series data, which have received\n",
      "little attention so far. To address this gap, we introduce JoLT to jointly learn desired\n",
      "representations from pre-trained time-series and text models. JoLT utilizes a\n",
      "Querying Transformer (Q-Former) to align the time-series and text representations.\n",
      "Our experiments on a large real-world electrocardiography dataset for medical time-\n",
      "series summarization show that JoLT outperforms state-of-the-art image captioning\n",
      "and medical question-answering approaches, and that the decoder architecture, size,\n",
      "and pre-training data can vary the performance on said tasks.\n",
      "1 Introduction\n",
      "Time-series and text data are frequently recorded in routine clinical care. But unlike general text\n",
      "or time-series, clinical data can only be analyzed by medical professionals, who spend substantial\n",
      "amounts of time analyzing biosignals, and entering summaries into electronic health records, away\n",
      "from direct patient care.\n",
      "To cater to an ever-increasing need to effectively and efficiently interpret clinical waveforms and text\n",
      "data, numerous studies have been devoted to automating clinical time-series and text interpretation.\n",
      "However, existing studies suffer from three key limitations. First, most existing studies model time-\n",
      "    \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mPlease provide a summary of the following article:\n",
      "    Article: However, existing studies suffer from three key limitations. First, most existing studies model time-\n",
      "series and text independently, even when these modalities frequently co-exist, e.g., electrocardiogram\n",
      "(ECG) and clinical description of findings. Second, the few studies that jointly model time-series\n",
      "and text are primarily rule-based, and do not offer the fluency and versatility associated with neural\n",
      "approaches. Third, most existing multi-modal methods do not explicitly model time-series data,\n",
      "instead converting it to graphs or images and using graph or computer vision models, respectively.\n",
      "We introduce JoLT ,Jointly Learned Representations of Language and Time-series, a neural model\n",
      "which can generate text given time-series and textual prompts as input. We evaluate JoLT on a\n",
      "medical time-series summarization problem on the PTB-XL dataset, and compare it with a state-of-\n",
      "the-art image captioning model, BLIP-2 [ 1]. To the best of our knowledge, JoLT is one of the first\n",
      "automated ECG interpretation methods that explicitly models time-series to generate meaningful\n",
      "textual interpretations. Our experiments show that explicitly modeling time-series data can improve\n",
      "time-series summarization performance over state-of-the-art approaches pre-trained on vast amounts\n",
      "of data, and can enable tasks that can be obscure for time-series and text multi-modal data, like\n",
      "question answering.\n",
      "1st Workshop on Deep Generative Models for Health at NeurIPS 2023.\n",
      "    \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mPlease provide a summary of the following article:\n",
      "    Article: Time-series\n",
      "EncoderQ-Former\n",
      "T ext...Language\n",
      "ModelGenerative Learning\n",
      "sinus rhythm type left\n",
      "bundle branch block\n",
      "Queries\n",
      "Representation\n",
      "LearningTime-series Pre-trainingFigure 1: An overview of JoLT . Given a time-series and an optional textual prompt as input, JoLT\n",
      "produces text as output. We pre-train a Transformer using the masked time-series reconstruction\n",
      "objective to use as an encoder, and the pre-trained language model as the decoder. The Q-Former\n",
      "is trained to align time-series and text representations. Learnable query tokens are used to extract\n",
      "time-series features conditioned on textual prompts.\n",
      "2 Related Work\n",
      "Time-series and Text Multimodal Models. Numerous studies have explored the problem of learning\n",
      "multimodal representations of data, such as graphs [ 2], image [ 1], and tabular data [ 3], grounded in\n",
      "text [ 4]. However, the challenge of jointly modeling time-series and text data has been relatively\n",
      "unexplored, primarily due to the lack of large publicly available paired time-series and text (pre-)\n",
      "training data, i.e., there is no equivalent of LAION-5B [ 5] or MS-COCO [ 6]. On the contrary, most\n",
      "existing time-series datasets are domain-specific, e.g. ECG interpretation [ 7] or stock price variations.\n",
      "This is exacerbated by the fact that most existing models are either statistical or rule-based, and\n",
      "necessitating substantial domain expertise that does not readily transfer across different domains [ 8].\n",
      "Clinical Text Summarization. In the healthcare domain, numerous studies have underscored the\n",
      "importance of developing automated text summarization systems. For instance, [ 9] highlights the\n",
      "pressing need for automated clinical report generation to alleviate the time burden on medical\n",
      "professionals, allowing them to focus more on patient care. Another relevant work by Harris and Zaki\n",
      "[10] introduced a CNN-LSTM framework designed to generate summaries of personal health data,\n",
      "such as heart rate, step count, and nutrient intake. Their approach demonstrated reasonably good\n",
      "    \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mPlease provide a summary of the following article:\n",
      "    Article: such as heart rate, step count, and nutrient intake. Their approach demonstrated reasonably good\n",
      "performance in this context. However, it is worth noting that the quality of the generated summaries\n",
      "heavily relied on the paired training texts, which were created using rule-based methods. We expect\n",
      "neural methods to outperform neural methods relying on rule-based methods.\n",
      "3 Problem Formulation and Methods\n",
      "3.0.1 Time-series Summarization.\n",
      "Given a time-series T ∈RC×Lof length LwithCchannels, our goal is to generate a textual\n",
      "interpretation of salient time-series features in the context of a target domain.\n",
      "3.0.2 Model.\n",
      "JoLT comprises of a time-series encoder, a text decoder, and a transformer model which ties these\n",
      "two unimodal components together (Fig. 1) [ 11]. The time-series encoder is a transformer model\n",
      "which treats time-series sub-sequences as input tokens. Our best model uses the Open Pre-trained\n",
      "Trained (OPT) language model as a decoder, although we also evaluate the model with various other\n",
      "decoder models[ 12]. To align time-series and text representations, we leverage Querying Transformer\n",
      "(Q-Former) introduced by Li et al. [1].\n",
      "Pre-training Time-series Encoder. First, we break the input time-series into disjoint sub-sequences\n",
      "called patches. A small percentage of these patches are masked uniformly at random and then fed\n",
      "into the encoder, which is trained to reconstruct the masked patches using the Mean Squared Error\n",
      "loss.\n",
      "Representation Learning. In this stage, we freeze the pre-trained encoder and train the Q-Former\n",
      "to learn query embeddings that capture salient time-series representations that are informative of\n",
      "2\n",
      "    \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mPlease provide a summary of the following article:\n",
      "    Article: sinus rhythm position type normal left bundle branch block left\n",
      "hypertrophy possible 4.46 unconfirmed reportGround\n",
      "Truth\n",
      "sinus rhythm. normal ecgFine-tuned\n",
      "BLIP-2\n",
      "sinus rhythm left type left bundle branch block left hypertrophy\n",
      "possible 4.46 unconfirmed reportJoLT\n",
      "(OPT-2.7B)\n",
      "Table 1: Qualitative evaluation of the results generated by JoLT compared to fine-tuned BLIP-2.\n",
      "JoLT (with OPT-2.7B decoder) generates text summaries very similar to the ground truth, while the\n",
      "fine-tuned BLIP-2 got the base class correct, but incorrectly described the time-series sample.\n",
      "input text. The Q-Former is trained using three objectives: (1) a contrastive loss to align time-series\n",
      "and text representations by maximizing their mutual information, (2) a text generation loss to train\n",
      "the Q-Former to generate text conditioned on input time-series, and a (3) time-series text matching\n",
      "lossfor finer grained alignment between time-series and text representations.\n",
      "Generative Learning. In this stage, we finally connect the frozen time-series encoder and Q-\n",
      "Former, with the frozen decoder, to leverage its generative capability. The query embeddings serve as\n",
      "soft prompts to guide the decoder’s language generation. We train the model end-to-end using the\n",
      "causal language modeling loss.\n",
      "4 Case Study: ECG Interpretation\n",
      "Dataset. We conduct an experiment on the PTB-XL dataset [ 7] to evaluate JoLT ’s ability to\n",
      "generate meaningful clinical interpretations from ECG waveform data. The dataset comprises of\n",
      "21,837 12-lead, 10 seconds long ECG recordings collected from 18,885 patients. A subset of ECG\n",
      "recordings is paired with gold-standard clinical interpretation, which we use to train and fine-tune\n",
      "our model. The train, validation, and test sets contain 11,319, 1,636, and 1,650 samples of paired\n",
      "time-series and text, respectively.\n",
      "Experimental Setup. We compare JoLT with the state-of-the-art image captioning model BLIP-2\n",
      "as baseline. We use the Matplotlib package1to transform time-series into graphical images before\n",
      "    \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mPlease provide a summary of the following article:\n",
      "    Article: as baseline. We use the Matplotlib package1to transform time-series into graphical images before\n",
      "feeding them into BLIP-2 . We evaluate BLIP-2 in a zero-shot setting. Since the models are not\n",
      "pre-trained on medical data, we also compare JoLT with BLIP-2 fine-tuned on the PTB-XL dataset.\n",
      "We evaluate multiple metrics that are commonly used to evaluate text generation performance.\n",
      "We further run ablation experiments to evaluate the impact of the decoder on our model’s performance.\n",
      "Specifically, we evaluate different architectures (e.g. GPT-2-Large versus OPT), of different sizes\n",
      "(e.g. OPT-2.7B versus OPT-6.7B ), and pre-trained on different data (e.g. BioGPT-Large versus\n",
      "GPT-2-Large ). Beyond ECG interpretation, we also run preliminary experiments to explore our\n",
      "JoLT ’s ability to solve multiple choice questions conditioned on time-series, on the PTB-XL dataset.\n",
      "To the best of our knowledge, this unique but important problem has not been explored in prior work.\n",
      "We compare JoLT against BiomedCLIP [13] as a baseline.\n",
      "Tables 1, 2, 3, and 4 summarize the results of our experiments. Below, we highlight some key\n",
      "observations.\n",
      "Domain-specific fine-tuning is critical for clinical waveform interpretation. Poor performance\n",
      "of off-the-shelf BLIP-2 with respect to its fine-tuned counterpart shows the need for domain-specific\n",
      "fine-tuning, at least within the clinical domain. This motivates the need for publicly available large\n",
      "paired time-series and text datasets and models.\n",
      "1https://pypi.org/project/matplotlib/\n",
      "3\n",
      "    \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mPlease provide a summary of the following article:\n",
      "    Article: Model Fine-tuned Rouge-1 Rouge-2 Rouge-LMETEOR BLEURTR P F 1 R P F 1 R P F 1\n",
      "BLIP-2 × 0.014 0.027 0.016 0.000 0.001 0.001 0.014 0.026 0.016 0.048 -1.283\n",
      "BLIP-2 ✓ 0.217 0.343 0.227 0.100 0.193 0.107 0.215 0.341 0.225 0.202 -0.930\n",
      "JoLT (OPT-2.7B) ✓ 0.404 0.528 0.436 0.277 0.355 0.295 0.403 0.526 0.435 0.414 -0.502\n",
      "Table 2: JoLT (with OPT-2.7B decoder) outperforms zero-shot and fine-tuned BLIP-2 baselines\n",
      "for the ECG interpretation task on the PTB-XL dataset. R,P, and F1denote the Recall, Precision,\n",
      "and F 1score.\n",
      "Decoder Rouge-1 Rouge-2 Rouge-LMETEOR BLEURTR P F 1 R P F 1 R P F 1\n",
      "OPT-2.7B 0.404 0.528 0.436 0.277 0.355 0.295 0.403 0.526 0.435 0.414 -0.502\n",
      "OPT-6.7B 0.400 0.518 0.429 0.277 0.350 0.294 0.399 0.517 0.428 0.408 -0.499\n",
      "GPT-2 0.113 0.420 0.169 0.020 0.080 0.029 0.113 0.419 0.168 0.017 -0.885\n",
      "BioGPT 0.107 0.342 0.153 0.022 0.072 0.031 0.107 0.342 0.153 0.212 -1.122\n",
      "BioMedLM 0.118 0.390 0.164 0.003 0.009 0.004 0.112 0.380 0.158 0.136 -1.079\n",
      "Table 3: JoLT with OPT-2.7B decoder outperforms JoLT with other decoders with different pre-\n",
      "trained data, different sizes, and different architectures on the ECG interpretation task on the PTB-XL\n",
      "dataset. R,P, and F1denote the Recall, Precision, and F 1score.\n",
      "Model Accuracy Weighted Precision Weighted Recall Weighted F1Score\n",
      "BiomedCLIP 0.11 0.57 0.11 0.02\n",
      "JoLT (OPT-2.7B) 0.54 0.31 0.54 0.4\n",
      "Table 4: Time-series conditioned multiple-choice question answering results on accuracy, precision,\n",
      "recall and F1-score. JoLT substantially outperforms BiomedCLIP , when given the prompt “ Which of\n",
      "the following five diagnostic classes does the following ECG belong to? \". Weighted averages are\n",
      "measured with respect to the support for each class.\n",
      "Explicitly modeling time-series improves summarization performance. JoLT produces textual\n",
      "summaries which are closer to ground truth compared to BLIP-2. We believe that the difference in\n",
      "performance largely stems from JoLT ’s ability to capture salient time-series features.\n",
      "    \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mPlease provide a summary of the following article:\n",
      "    Article: performance largely stems from JoLT ’s ability to capture salient time-series features.\n",
      "Time-series and text joint modeling helps improve upon baselines for multi-class question\n",
      "answering. JoLT outperforms BiomedCLIP on time-series conditioned multiple choice question\n",
      "answering problem. However, we note that both the models perform poorly on this task. We believe\n",
      "that future work should carefully look to improve our model’s performance on this important task,\n",
      "with both models overconfidently predicting the majority class.\n",
      "Broken assumptions. For our ablation experiments, we hypothesized that: (1) Language models\n",
      "with more parameters will be better at text generation than smaller models, and (2) models pre-trained\n",
      "on clinical data ( BioGPT ,BiomedLM ) will be better than those trained on general text data ( GPT-2 ).\n",
      "However, our experiments did not support any of these hypothesis. The former can be partly explained\n",
      "by the fact that clinical interpretations are terse and do not require fluent large language models. We\n",
      "believe that further experiments are necessary to conclusively accept or reject the latter hypothesis.\n",
      "5 Conclusion and Future Work\n",
      "In this work, we introduced JoLT , a jointly model text and time-series data with a focus on ECG\n",
      "interpretation. We evaluated our model against state-of-the-art image captioning models in the context\n",
      "of clinical summarization. It’s worth noting a crucial aspect of our approach: the encoder of JoLT\n",
      "was pre-trained using a relatively small set of time-series data from the PTB-XL dataset. We posit\n",
      "that extending this pre-training phase to include a large amount of time-series data is likely to further\n",
      "improve its performance. Additionally, it’s important to acknowledge that the evaluation of these\n",
      "models presents significant challenges, as discussed in prior research [ 14]. Therefore, future research\n",
      "endeavors should aim to comprehensively and robustly assess the capabilities of such models in\n",
      "clinical applications.\n",
      "4\n",
      "    \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mPlease provide a summary of the following article:\n",
      "    Article: 6 Acknowledgments\n",
      "This work was partially supported by the U.S. Army Research Office and the U.S. Army Futures\n",
      "Command under contract W911NF-20-F-0020.\n",
      "References\n",
      "[1]Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image\n",
      "pre-training with frozen image encoders and large language models. In ICML , 2023.\n",
      "[2]Anthony Colas, Mehrdad Alvandipour, and Daisy Zhe Wang. GAP: A graph-aware lan-\n",
      "guage model framework for knowledge graph-to-text generation. In Proceedings of the 29th\n",
      "International Conference on Computational Linguistics , pages 5755–5769, Gyeongju, Repub-\n",
      "lic of Korea, October 2022. International Committee on Computational Linguistics. URL\n",
      "https://aclanthology.org/2022.coling-1.506 .\n",
      "[3]Rong Ye, Wenxian Shi, Hao Zhou, Zhongyu Wei, and Lei Li. Variational template machine for\n",
      "data-to-text generation. In International Conference on Learning Representations , 2020. URL\n",
      "https://openreview.net/forum?id=HkejNgBtPB .\n",
      "[4]Mandar Sharma, Ajay Gogineni, and Naren Ramakrishnan. Innovations in neural data-to-text\n",
      "generation: A survey, 2023.\n",
      "[5]Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\n",
      "Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n",
      "5b: An open large-scale dataset for training next generation image-text models. Advances in\n",
      "Neural Information Processing Systems , 35:25278–25294, 2022.\n",
      "[6]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\n",
      "Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\n",
      "Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\n",
      "Proceedings, Part V 13 , pages 740–755. Springer, 2014.\n",
      "[7]Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Dieter Kreiseler, Fatima I. Lunze,\n",
      "Wojciech Samek, and Tobias Schaeffter. Ptb-xl, a large publicly available electrocardiography\n",
      "dataset. Scientific Data , 2020. doi: https://doi.org/10.1038/s41597-020-0495-6.\n",
      "    \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mPlease provide a summary of the following article:\n",
      "    Article: dataset. Scientific Data , 2020. doi: https://doi.org/10.1038/s41597-020-0495-6.\n",
      "[8]Siddharth Biswal, Cao Xiao, M. Brandon Westover, and Jimeng Sun. Eegtotext: Learning\n",
      "to write medical reports from eeg recordings. In Finale Doshi-Velez, Jim Fackler, Ken Jung,\n",
      "David Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiens, editors, Proceedings of the\n",
      "4th Machine Learning for Healthcare Conference , volume 106 of Proceedings of Machine\n",
      "Learning Research , pages 513–531. PMLR, 09–10 Aug 2019. URL https://proceedings.\n",
      "mlr.press/v106/biswal19a.html .\n",
      "[9]Steffen Pauws, Albert Gatt, Emiel Krahmer, and Ehud Reiter. Making effective use of healthcare\n",
      "data using data-to-text technology, 2018.\n",
      "[10] Jonathan Harris and Mohammed J. Zaki. Towards neural numeric-to-text generation from\n",
      "temporal personal health data, 2022.\n",
      "[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
      "Ł ukasz Kaiser, and Illia Polosukhin. Transformers: Attention is all you need. In I. Guyon,\n",
      "U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, ed-\n",
      "itors, Advances in Neural Information Processing Systems , volume 30. Curran Associates,\n",
      "Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/\n",
      "3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .\n",
      "[12] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\n",
      "Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam\n",
      "Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke\n",
      "Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.\n",
      "5\n",
      "    \u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mPlease provide a summary of the following article:\n",
      "    Article: [13] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh\n",
      "Rao, Mu Wei, Naveen Valluri, Cliff Wong, Matthew P. Lungren, Tristan Naumann, and Hoifung\n",
      "Poon. Large-scale domain-specific pretraining for biomedical vision-language processing, 2023.\n",
      "[14] Michael Wornow, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming,\n",
      "Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. The shaky foundations of large lan-\n",
      "guage models and foundation models for electronic health records. npj Digital Medicine ,\n",
      "6(1):135, Jul 2023. ISSN 2398-6352. doi: 10.1038/s41746-023-00879-8. URL https:\n",
      "//doi.org/10.1038/s41746-023-00879-8 .\n",
      "6\n",
      "    \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3713 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Provide the final summary of the entire article with the important points.\n",
      "    Add a Motivation Title,Start the precise summary with an introduction and provide the summary in number \n",
      "    points for the article.\n",
      "    Article:This article introduces **JoLT**, a novel method for jointly learning representations from time-series and text data. \n",
      "\n",
      "**The Problem:**\n",
      "\n",
      "* Time-series and text data are abundant in healthcare (e.g., ECG reports) but are often treated separately.\n",
      "* Existing joint modeling methods convert time-series into images or graphs, which can be inefficient and lose inherent temporal information.\n",
      "* Tasks like summarizing and answering questions about time-series data in healthcare are under-explored.\n",
      "\n",
      "**JoLT's Solution:**\n",
      "\n",
      "* **Joint Representation Learning:** JoLT leverages pre-trained models for both time-series and text data and uses a Querying Transformer (Q-Former) to align their representations. This allows for a deeper understanding of the relationship between the two modalities.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Improved Performance:** Experiments on a large ECG dataset demonstrate that JoLT outperforms state-of-the-art methods that rely on image captioning or medical question answering approaches for time-series summarization.\n",
      "* **Flexibility:** The study shows that JoLT's performance can be further enhanced by tuning the decoder architecture, size, and pre-training data.\n",
      "\n",
      "**Impact:**\n",
      "\n",
      "* JoLT opens up new possibilities for effectively analyzing and understanding complex healthcare data by jointly considering both time-series and textual information. This can lead to improved clinical decision-making, automated report generation, and more efficient patient care.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This article introduces **JoLT**, a novel neural model designed to jointly learn representations from both time-series data and text. \n",
      "\n",
      "The authors highlight three key limitations of existing methods for handling multi-modal data, particularly in the medical domain:\n",
      "\n",
      "1. **Independent Modeling:** Most studies treat time-series and text separately, even when they often co-exist (e.g., ECG readings and clinical descriptions).\n",
      "2. **Rule-Based Approaches:**  Limited attempts to integrate both modalities rely on rules rather than flexible neural models.\n",
      "3. **Indirect Time-Series Representation:** Many methods convert time-series into graphs or images, losing the inherent temporal information.\n",
      "\n",
      "JoLT addresses these limitations by directly modeling time-series data and learning a joint representation with text. The authors evaluate JoLT on medical time-series summarization using the PTB-XL dataset, comparing it to a state-of-the-art image captioning model (BLIP-2). \n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "* JoLT achieves improved performance over BLIP-2, demonstrating the advantage of explicitly modeling time-series data for this task.\n",
      "* JoLT opens up possibilities for new tasks involving both time-series and text, such as question answering.\n",
      "* To the best of the authors' knowledge, JoLT is one of the first automated ECG interpretation methods that directly handles time-series data for generating textual interpretations.\n",
      "\n",
      "\n",
      "Overall, the article presents JoLT as a promising new approach for multi-modal analysis, particularly in healthcare, by effectively integrating time-series and text data within a neural framework. \n",
      "\n",
      "\n",
      "This article proposes a novel method called JoLT (Joint Learning of Time-series and Text) for jointly modeling time-series and text data. \n",
      "\n",
      "**Key points:**\n",
      "\n",
      "* **Problem:**  There's a lack of large, publicly available datasets for training models on paired time-series and text data, making it difficult to develop effective multimodal models for this domain.\n",
      "* **Solution:** JoLT utilizes a Transformer encoder pre-trained on a masked time-series reconstruction objective and a pre-trained language model as the decoder. A Q-Former architecture is introduced to align time-series and text representations, using learnable query tokens to extract relevant time-series features based on textual prompts.\n",
      "* **Motivation:**  The authors highlight the importance of joint modeling for time-series and text data, particularly in the healthcare domain, where clinical text summarization is crucial for alleviating the workload of medical professionals. \n",
      "\n",
      "**Related Work:**\n",
      "\n",
      "* The article briefly mentions existing research on multimodal learning for graphs, images, and tabular data, but emphasizes the relative lack of work on time-series and text.\n",
      "* It also discusses the importance of clinical text summarization and cites relevant work on generating summaries from personal health data.\n",
      "\n",
      "\n",
      "Overall, the article introduces a promising new approach to tackling the challenge of jointly modeling time-series and text data, potentially paving the way for more sophisticated applications in healthcare and other domains. \n",
      "\n",
      "\n",
      "This article excerpt describes a novel approach to **time-series summarization** called **JoLT**. \n",
      "\n",
      "**JoLT's goal is to generate textual descriptions of key features within time-series data**, such as heart rate, step count, or nutrient intake, tailored to a specific domain.\n",
      "\n",
      "Here's a breakdown of the key points:\n",
      "\n",
      "* **Model Architecture:** JoLT utilizes a transformer-based architecture consisting of a time-series encoder, a text decoder, and a **Querying Transformer (Q-Former)** to bridge the gap between these components.\n",
      "* **Time-series Encoder:** This component, pre-trained using the Open Pre-trained Transformer (OPT) model, processes time-series data by breaking it into sub-sequences called \"patches.\" It learns to reconstruct masked patches, enabling it to capture essential temporal patterns.\n",
      "* **Text Decoder:** Various decoder models, including OPT, are employed to generate the textual summary based on the encoded time-series representations.\n",
      "* **Q-Former:** This module aligns the time-series and text representations, ensuring that the generated summary accurately reflects the salient features of the input data.\n",
      "* **Performance:** While the article acknowledges the promising performance of JoLT, it highlights the dependence on rule-based paired training texts for generating summaries. The authors anticipate that future advancements in neural methods will further enhance the model's capabilities.\n",
      "\n",
      "\n",
      "Overall, JoLT presents a promising framework for automatically generating meaningful textual interpretations of time-series data, paving the way for more intuitive and insightful data analysis.\n",
      "\n",
      "\n",
      "\n",
      "This article describes JoLT, a novel model for generating textual descriptions from time-series data. \n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **JoLT's Architecture:**  It combines a time-series encoder with a text generation module called Q-Former. The Q-Former is trained using three objectives: contrastive learning to align time-series and text representations, text generation to produce text based on time-series, and a time-series text matching loss for finer alignment.\n",
      "* **Generative Learning:** A frozen time-series encoder and Q-Former are connected to a frozen decoder for text generation. Query embeddings act as soft prompts to guide the decoder.\n",
      "* **Case Study: ECG Interpretation:** JoLT is evaluated on the PTB-XL dataset, which contains ECG recordings paired with clinical interpretations. \n",
      "\n",
      "**Results:**\n",
      "\n",
      "* JoLT outperforms the fine-tuned BLIP-2 image captioning model in generating accurate and meaningful clinical interpretations from ECG data. \n",
      "* The article provides a table comparing JoLT's output to both ground truth and BLIP-2's results, highlighting JoLT's superior performance.\n",
      "\n",
      "**Method:**\n",
      "\n",
      "The article explains how ECG data is transformed into graphical images using Matplotlib and then used as input for both JoLT and BLIP-2.\n",
      "\n",
      "\n",
      "In essence, the article presents JoLT as a promising new approach for automatically generating textual descriptions from time-series data, with particular success in the domain of medical diagnostics.\n",
      "\n",
      "\n",
      "This article explores the use of a model called JoLT for interpreting clinical waveforms, specifically focusing on ECGs. \n",
      "\n",
      "Here are the key takeaways:\n",
      "\n",
      "* **JoLT's Approach:** JoLT combines a vision model (BLIP-2) with a language model to analyze time-series data like ECGs and generate textual interpretations.\n",
      "* **Evaluation:** The authors evaluate JoLT's performance in a zero-shot setting (without any prior training on medical data) and compare it to a fine-tuned version trained on the PTB-XL dataset. They also compare JoLT to a baseline model, BiomedCLIP.\n",
      "* **Findings:**\n",
      "    * **Fine-tuning is crucial:**  A JoLT model fine-tuned on medical data significantly outperforms the off-the-shelf version, highlighting the importance of domain-specific training for clinical tasks.\n",
      "    * **Exploring new applications:** The authors also investigate JoLT's ability to answer multiple-choice questions about time-series data, a novel application not previously explored.\n",
      "\n",
      "* **Future Directions:** The authors emphasize the need for publicly available large datasets of paired time-series and textual data for clinical applications, which would facilitate the development and improvement of models like JoLT.\n",
      "\n",
      "\n",
      "Overall, this article demonstrates the potential of JoLT for interpreting clinical waveforms and calls for further research and data sharing in this area. \n",
      "\n",
      "\n",
      "This article presents **JoLT**, a novel model for text summarization of time-series data, specifically focusing on ECG interpretation. \n",
      "\n",
      "**Key findings:**\n",
      "\n",
      "* **JoLT outperforms both zero-shot and fine-tuned BLIP-2 baselines** on the PTB-XL dataset for ECG interpretation, achieving higher Rouge scores, BLEURT scores, and F1-scores.\n",
      "* **JoLT also surpasses other decoders** like OPT-6.7B, GPT-2, BioGPT, and BioMedLM in terms of performance on the same task.\n",
      "* **JoLT's advantage stems from its explicit modeling of time-series features.** This allows it to generate textual summaries that are closer to the ground truth compared to BLIP-2, which lacks this capability.\n",
      "\n",
      "**Experiments:**\n",
      "\n",
      "The article compares JoLT to various baselines using metrics like Rouge, BLEURT, Precision, Recall, and F1-score for ECG interpretation. It also demonstrates JoLT's superiority in time-series conditioned multiple-choice question answering, outperforming BiomedCLIP significantly.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "JoLT demonstrates the effectiveness of explicitly modeling time-series data for text summarization tasks. Its ability to capture salient time-series features leads to improved performance compared to models that do not consider this aspect. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This article presents JoLT, a novel model designed to jointly process text and time-series data, with a specific focus on interpreting electrocardiogram (ECG) data. \n",
      "\n",
      "**Key findings:**\n",
      "\n",
      "* **JoLT's performance stems from its ability to capture relevant features from both text and time-series data.** \n",
      "* **While JoLT outperforms BiomedCLIP on a specific ECG-related question answering task, both models struggle overall.** This suggests a need for further research to improve performance on this challenging task.\n",
      "* **Initial hypotheses about the impact of model size and pre-training data on performance were not supported.**  Larger language models and those pre-trained on clinical data did not necessarily perform better. This highlights the need for more nuanced understanding of model performance in clinical contexts.\n",
      "\n",
      "**Future directions:**\n",
      "\n",
      "* **Expand pre-training data for JoLT:**  Including a larger dataset of time-series data is likely to enhance its performance.\n",
      "* **Develop more robust evaluation methods:**  Current methods for evaluating models in clinical settings are limited. Research should focus on creating more comprehensive and reliable evaluation techniques.\n",
      "* **Investigate the impact of data characteristics:**  The nature of clinical data, particularly its brevity and specificity, may require different model architectures and training strategies.\n",
      "\n",
      "\n",
      "\n",
      "Overall, the article presents a promising new approach to ECG interpretation but also highlights the significant challenges and opportunities for future research in this field. \n",
      "\n",
      "\n",
      "This document appears to be the acknowledgments and references section of a research paper. \n",
      "\n",
      "**Acknowledgments:**\n",
      "\n",
      "* The research was partially funded by the U.S. Army Research Office and the U.S. Army Futures Command under contract W911NF-20-F-0020.\n",
      "\n",
      "**References:**\n",
      "\n",
      "The document lists seven references, which are likely papers or datasets cited within the research. These include:\n",
      "\n",
      "* **BLIP-2:** A paper describing a method for bootstrapping language-image pre-training.\n",
      "* **GAP:** A paper presenting a graph-aware language model framework for knowledge graph-to-text generation.\n",
      "* **Variational Template Machine:** A paper on a variational template machine for data-to-text generation.\n",
      "* **Survey on Neural Data-to-Text Generation:** A survey paper on innovations in neural data-to-text generation.\n",
      "* **Laion-5b:** A paper describing an open large-scale dataset for training image-text models.\n",
      "* **Microsoft COCO:** A paper describing the Microsoft COCO dataset of common objects in context.\n",
      "* **PTB-XL:** A paper describing a large publicly available electrocardiography dataset. \n",
      "\n",
      "\n",
      "\n",
      "Without the main body of the paper, it's difficult to understand the specific context of these references and how they relate to the research findings. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Provide the final summary of the entire article with the important points.\n",
      "    Add a Motivation Title,Start the precise summary with an introduction and provide the summary in number \n",
      "    points for the article.\n",
      "    Article:This text appears to be a section of a larger document referencing several research papers and datasets.  Here's a breakdown of the information provided:\n",
      "\n",
      "**Key Papers & Datasets:**\n",
      "\n",
      "* **[8] Eegtotext:** This paper introduces a method called \"Eegtotext\" that leverages Electroencephalogram (EEG) recordings to automatically generate medical reports. It was presented at the 4th Machine Learning for Healthcare Conference in 2019.\n",
      "* **[9] Making Effective Use of Healthcare Data:** This paper explores how \"Data-to-Text\" technology can be effectively utilized for processing and generating text from healthcare data.\n",
      "* **[10] Neural Numeric-to-Text Generation:** This paper focuses on using neural networks to convert temporal personal health data into human-readable text. \n",
      "* **[11] Transformers: Attention is All You Need:** This seminal paper introduced the \"Transformer\" architecture, a revolutionary deep learning model that relies on \"attention\" mechanisms and has become foundational for many modern natural language processing (NLP) tasks.\n",
      "* **[12] Open-Pre-trained Transformer Language Models (OPT):** This paper describes the development of OPT, a family of open-source transformer language models that are publicly available for research and development.\n",
      "\n",
      "**Themes:**\n",
      "\n",
      "The referenced papers and dataset highlight several important trends in the field of healthcare AI:\n",
      "\n",
      "* **Data-Driven Medical Reporting:**  Using data like EEG recordings or personal health data to automatically generate medical reports.\n",
      "* **NLP for Healthcare:** Applying natural language processing techniques to understand, analyze, and generate text related to healthcare.\n",
      "* **Transformer Models:** The increasing reliance on transformer models like BERT and GPT for various NLP tasks, including those relevant to healthcare.\n",
      "* **Open-Source AI:** The growing trend of making AI models and datasets publicly accessible to foster collaboration and innovation.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like me to elaborate on any specific paper or dataset!\n",
      "\n",
      "\n",
      "The two provided articles discuss the challenges and opportunities of applying large language models (LLMs) and foundation models to biomedical vision-language processing (VLP) and electronic health records (EHRs).\n",
      "\n",
      "**Article [13]:** This paper introduces **\"Large-scale domain-specific pretraining for biomedical vision-language processing\"**.  \n",
      "\n",
      "*  The authors address the limitations of using general-purpose LLMs for biomedical VLP tasks due to their lack of domain-specific knowledge. \n",
      "* They propose a novel approach of pretraining LLMs on a large-scale biomedical dataset, leading to improved performance on downstream VLP tasks like image captioning and question answering.\n",
      "\n",
      "**Article [14]:** This article, titled **\"The shaky foundations of large language models and foundation models for electronic health records\"**, takes a more critical stance.\n",
      "\n",
      "* The authors highlight several **concerns regarding the use of LLMs and foundation models in healthcare**. \n",
      "* They point out the potential for **biases, inaccuracies, and ethical issues** arising from these models' training data and their lack of transparency.\n",
      "*  The authors emphasize the need for **robust evaluation, explainability, and ethical considerations** before widespread adoption of these models in clinical settings.\n",
      "\n",
      "\n",
      "**In summary:** While [13] focuses on the potential benefits of domain-specific pretraining for biomedical VLP, [14] raises important concerns about the risks and challenges associated with using LLMs in healthcare, particularly with EHRs. Both articles contribute to the ongoing discussion about the responsible and effective application of AI in the complex domain of medicine.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Provide the final summary of the entire article with the important points.\n",
      "    Add a Motivation Title,Start the precise summary with an introduction and provide the summary in number \n",
      "    points for the article.\n",
      "    Article:You've done a fantastic job summarizing the article and extracting the key points! Your summaries are clear, concise, and accurately reflect the main ideas presented. \n",
      "\n",
      "Here are a few observations and suggestions:\n",
      "\n",
      "* **Consistency:** Maintaining a consistent structure for each summary (e.g., starting with a brief introduction, followed by numbered points) enhances readability. \n",
      "* **Specificity:**  In some summaries, you could delve a bit deeper into the specific contributions of JoLT. For example, mentioning the type of transformer architecture used or the specific pre-training objectives could provide more technical detail.\n",
      "* **Motivation:**  You've effectively highlighted the motivation behind JoLT in several summaries, emphasizing the need for effective joint modeling of time-series and text data, particularly in healthcare.  \n",
      "\n",
      "**Overall, your summaries are well-written and insightful. They provide a comprehensive overview of the key aspects of the article and its contributions.**\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "##  The Promise and Peril of AI in Healthcare: A Critical Look at LLMs\n",
      "\n",
      "This summary explores the potential benefits and inherent risks of applying large language models (LLMs) and foundation models to healthcare, specifically in biomedical vision-language processing (VLP) and electronic health records (EHRs).\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "1. **Domain-Specific Pretraining:**  Article [13] argues that pretraining LLMs on large biomedical datasets can significantly improve their performance in tasks like image captioning and question answering within the healthcare context.\n",
      "\n",
      "2. **Bridging the Gap in Biomedical VLP:**  The authors of [13] highlight the limitations of using general-purpose LLMs for biomedical VLP due to their lack of specialized knowledge. Domain-specific pretraining aims to overcome this gap.\n",
      "\n",
      "3. **Concerns about LLMs in Healthcare:** Article [14] raises serious concerns about the use of LLMs and foundation models in healthcare, particularly with sensitive EHR data.\n",
      "\n",
      "4. **Potential for Bias and Inaccuracy:**  The authors of [14] point out the risk of biases and inaccuracies embedded in the training data of LLMs, which could lead to flawed medical diagnoses or treatment recommendations.\n",
      "\n",
      "5. **Ethical Considerations:** The lack of transparency in how LLMs arrive at their outputs raises significant ethical questions regarding accountability and patient trust in healthcare settings.\n",
      "\n",
      "6. **Need for Robust Evaluation and Explainability:**  [14] emphasizes the urgent need for rigorous evaluation methods and techniques that can explain the reasoning behind LLM-generated outputs in healthcare.\n",
      "\n",
      "7. **Responsible AI Adoption:** Both articles contribute to the ongoing debate about the responsible and ethical integration of AI into healthcare. While LLMs offer promising possibilities, their deployment requires careful consideration of potential risks and the implementation of safeguards to ensure patient safety and well-being. \n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'##  Navigating the Dual Nature of AI in Healthcare: Promises and Perils\\n\\nThis summary explores the potential benefits and inherent risks associated with applying large language models (LLMs) and foundation models in healthcare, focusing on their impact on biomedical vision-language processing (VLP) and electronic health records (EHRs).\\n\\n**Key Points:**\\n\\n1. **Enhanced Biomedical VLP:**  Research suggests that pretraining LLMs on large biomedical datasets can significantly improve their performance in tasks like image captioning and question answering within the healthcare domain. This specialized training addresses the limitations of using general-purpose LLMs for biomedical VLP.\\n\\n2. **Addressing Healthcare-Specific Challenges:** LLMs can be valuable tools for analyzing and interpreting EHRs, potentially aiding in diagnoses, treatment recommendations, and patient monitoring. However, their application in this sensitive domain requires careful consideration due to ethical and safety concerns.\\n\\n3. **Bias and Inaccuracy Concerns:** LLMs trained on potentially biased datasets could perpetuate existing inequalities in healthcare or generate inaccurate medical information, leading to misdiagnosis or inappropriate treatment.\\n\\n4. **Ethical Implications and Transparency:** The lack of transparency in how LLMs arrive at their outputs raises ethical questions regarding accountability and patient trust. Ensuring explainability and clear decision-making processes is crucial for responsible AI implementation in healthcare.\\n\\n5. **Need for Robust Evaluation and Regulation:** Rigorous evaluation methods and transparent regulations are essential for assessing the safety and efficacy of LLMs in healthcare. Continuous monitoring and adaptation are necessary to mitigate potential risks and ensure patient well-being. \\n\\n\\n\\nThe article emphasizes a cautious yet optimistic approach to AI in healthcare. While LLMs offer promising advancements, addressing the ethical, safety, and bias concerns is paramount for their successful and responsible integration into clinical practice. \\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = summary_chain.run(splitted_docs)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refine Chain Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"JoLT: Jointly Learned Representations of\n",
      "Language and Time-Series\n",
      "Yifu Cai, Mononito Goswami, Arjun Choudhry, Arvind Srinivasan, Artur Dubrawski\n",
      "Auton Lab, School of Computer Science, Carnegie Mellon University\n",
      "Pittsburgh, PA 15213\n",
      "arvind.srini.8@gmail.com ,{yifuc, mgoswami, arjuncho, awd}@andrew.cmu.edu\n",
      "Abstract\n",
      "Time-series and text data is prevalent in healthcare and frequently exist in tandem,\n",
      "for e.g., in electrocardiogram (ECG) interpretation reports. Yet, these modalities\n",
      "are typically modeled independently. Even studies that jointly model time-series\n",
      "and text do so by converting time-series to images or graphs. We hypothesize\n",
      "that explicitly modeling time-series jointly with text can improve tasks such as\n",
      "summarization and question answering for time-series data, which have received\n",
      "little attention so far. To address this gap, we introduce JoLT to jointly learn desired\n",
      "representations from pre-trained time-series and text models. JoLT utilizes a\n",
      "Querying Transformer (Q-Former) to align the time-series and text representations.\n",
      "Our experiments on a large real-world electrocardiography dataset for medical time-\n",
      "series summarization show that JoLT outperforms state-of-the-art image captioning\n",
      "and medical question-answering approaches, and that the decoder architecture, size,\n",
      "and pre-training data can vary the performance on said tasks.\n",
      "1 Introduction\n",
      "Time-series and text data are frequently recorded in routine clinical care. But unlike general text\n",
      "or time-series, clinical data can only be analyzed by medical professionals, who spend substantial\n",
      "amounts of time analyzing biosignals, and entering summaries into electronic health records, away\n",
      "from direct patient care.\n",
      "To cater to an ever-increasing need to effectively and efficiently interpret clinical waveforms and text\n",
      "data, numerous studies have been devoted to automating clinical time-series and text interpretation.\n",
      "However, existing studies suffer from three key limitations. First, most existing studies model time-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper introduces JoLT, a novel method for jointly learning representations from time-series and text data.  \n",
      "\n",
      "Unlike previous approaches that convert time-series into images or graphs, JoLT directly aligns time-series and text representations using a Querying Transformer (Q-Former). \n",
      "\n",
      "Experiments on a large electrocardiography dataset demonstrate that JoLT outperforms state-of-the-art methods for medical time-series summarization and question answering. The results highlight the effectiveness of directly modeling the relationship between time-series and text data for improving clinical interpretation tasks.  \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "However, existing studies suffer from three key limitations. First, most existing studies model time-\n",
      "series and text independently, even when these modalities frequently co-exist, e.g., electrocardiogram\n",
      "(ECG) and clinical description of findings. Second, the few studies that jointly model time-series\n",
      "and text are primarily rule-based, and do not offer the fluency and versatility associated with neural\n",
      "approaches. Third, most existing multi-modal methods do not explicitly model time-series data,\n",
      "instead converting it to graphs or images and using graph or computer vision models, respectively.\n",
      "We introduce JoLT ,Jointly Learned Representations of Language and Time-series, a neural model\n",
      "which can generate text given time-series and textual prompts as input. We evaluate JoLT on a\n",
      "medical time-series summarization problem on the PTB-XL dataset, and compare it with a state-of-\n",
      "the-art image captioning model, BLIP-2 [ 1]. To the best of our knowledge, JoLT is one of the first\n",
      "automated ECG interpretation methods that explicitly models time-series to generate meaningful\n",
      "textual interpretations. Our experiments show that explicitly modeling time-series data can improve\n",
      "time-series summarization performance over state-of-the-art approaches pre-trained on vast amounts\n",
      "of data, and can enable tasks that can be obscure for time-series and text multi-modal data, like\n",
      "question answering.\n",
      "1st Workshop on Deep Generative Models for Health at NeurIPS 2023.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here's a refined summary incorporating the new context:\n",
      "\n",
      "This paper introduces JoLT, a novel neural method for jointly learning representations from time-series and text data.  Existing approaches often struggle to effectively integrate these modalities, either treating them independently or relying on rule-based systems. JoLT directly aligns time-series and text representations using a Querying Transformer (Q-Former),  addressing the limitations of previous methods that convert time-series into images or graphs. \n",
      "\n",
      "Experiments on a large electrocardiography (ECG) dataset demonstrate that JoLT outperforms state-of-the-art methods for medical time-series summarization and question answering.  This highlights the effectiveness of directly modeling the relationship between time-series and text data for improving clinical interpretation tasks, enabling new capabilities like automated ECG interpretation. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**Key Changes:**\n",
      "\n",
      "* **Highlighted the limitations of previous approaches:**  The refined summary emphasizes the shortcomings of existing methods, making JoLT's novelty more apparent.\n",
      "* **Emphasized the direct alignment of modalities:**  The summary clarifies that JoLT directly works with time-series and text, unlike methods that convert time-series into other formats.\n",
      "* **Showcased the impact on clinical interpretation:**  The summary stresses the potential of JoLT for real-world applications in healthcare, specifically mentioning automated ECG interpretation. \n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like to explore any aspect of the summary further!\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Time-series\n",
      "EncoderQ-Former\n",
      "T ext...Language\n",
      "ModelGenerative Learning\n",
      "sinus rhythm type left\n",
      "bundle branch block\n",
      "Queries\n",
      "Representation\n",
      "LearningTime-series Pre-trainingFigure 1: An overview of JoLT . Given a time-series and an optional textual prompt as input, JoLT\n",
      "produces text as output. We pre-train a Transformer using the masked time-series reconstruction\n",
      "objective to use as an encoder, and the pre-trained language model as the decoder. The Q-Former\n",
      "is trained to align time-series and text representations. Learnable query tokens are used to extract\n",
      "time-series features conditioned on textual prompts.\n",
      "2 Related Work\n",
      "Time-series and Text Multimodal Models. Numerous studies have explored the problem of learning\n",
      "multimodal representations of data, such as graphs [ 2], image [ 1], and tabular data [ 3], grounded in\n",
      "text [ 4]. However, the challenge of jointly modeling time-series and text data has been relatively\n",
      "unexplored, primarily due to the lack of large publicly available paired time-series and text (pre-)\n",
      "training data, i.e., there is no equivalent of LAION-5B [ 5] or MS-COCO [ 6]. On the contrary, most\n",
      "existing time-series datasets are domain-specific, e.g. ECG interpretation [ 7] or stock price variations.\n",
      "This is exacerbated by the fact that most existing models are either statistical or rule-based, and\n",
      "necessitating substantial domain expertise that does not readily transfer across different domains [ 8].\n",
      "Clinical Text Summarization. In the healthcare domain, numerous studies have underscored the\n",
      "importance of developing automated text summarization systems. For instance, [ 9] highlights the\n",
      "pressing need for automated clinical report generation to alleviate the time burden on medical\n",
      "professionals, allowing them to focus more on patient care. Another relevant work by Harris and Zaki\n",
      "[10] introduced a CNN-LSTM framework designed to generate summaries of personal health data,\n",
      "such as heart rate, step count, and nutrient intake. Their approach demonstrated reasonably good\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper introduces JoLT, a novel neural method for jointly learning representations from time-series and text data.  Existing approaches often struggle to effectively integrate these modalities, either treating them independently or relying on rule-based systems that require significant domain expertise and may not generalize well across different domains. JoLT directly aligns time-series and text representations using a Querying Transformer (Q-Former), addressing the limitations of previous methods that convert time-series into images or graphs. \n",
      "\n",
      "Key innovations include:\n",
      "\n",
      "* **Direct Alignment:** JoLT directly works with time-series and text data, enabling a more natural and potentially more effective integration of the two modalities.\n",
      "* **Querying Transformer:** The Q-Former mechanism allows for selective extraction of time-series features based on textual prompts, enhancing the model's ability to focus on relevant information.\n",
      "* **Pre-training:** JoLT leverages pre-trained language models and a time-series encoder trained on masked reconstruction tasks, providing a strong foundation for learning joint representations.\n",
      "\n",
      "Experiments on a large electrocardiography (ECG) dataset demonstrate that JoLT outperforms state-of-the-art methods for medical time-series summarization and question answering. This highlights the effectiveness of directly modeling the relationship between time-series and text data for improving clinical interpretation tasks, enabling new capabilities like automated ECG interpretation.  \n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "such as heart rate, step count, and nutrient intake. Their approach demonstrated reasonably good\n",
      "performance in this context. However, it is worth noting that the quality of the generated summaries\n",
      "heavily relied on the paired training texts, which were created using rule-based methods. We expect\n",
      "neural methods to outperform neural methods relying on rule-based methods.\n",
      "3 Problem Formulation and Methods\n",
      "3.0.1 Time-series Summarization.\n",
      "Given a time-series T ∈RC×Lof length LwithCchannels, our goal is to generate a textual\n",
      "interpretation of salient time-series features in the context of a target domain.\n",
      "3.0.2 Model.\n",
      "JoLT comprises of a time-series encoder, a text decoder, and a transformer model which ties these\n",
      "two unimodal components together (Fig. 1) [ 11]. The time-series encoder is a transformer model\n",
      "which treats time-series sub-sequences as input tokens. Our best model uses the Open Pre-trained\n",
      "Trained (OPT) language model as a decoder, although we also evaluate the model with various other\n",
      "decoder models[ 12]. To align time-series and text representations, we leverage Querying Transformer\n",
      "(Q-Former) introduced by Li et al. [1].\n",
      "Pre-training Time-series Encoder. First, we break the input time-series into disjoint sub-sequences\n",
      "called patches. A small percentage of these patches are masked uniformly at random and then fed\n",
      "into the encoder, which is trained to reconstruct the masked patches using the Mean Squared Error\n",
      "loss.\n",
      "Representation Learning. In this stage, we freeze the pre-trained encoder and train the Q-Former\n",
      "to learn query embeddings that capture salient time-series representations that are informative of\n",
      "2\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper introduces JoLT, a novel neural method for jointly learning representations from time-series and text data.  Existing approaches often struggle to effectively integrate these modalities, either treating them independently or relying on rule-based systems that require significant domain expertise and may not generalize well across different domains.  Prior work, such as [citation omitted], attempted to integrate time-series and text data for tasks like activity summarization, but relied on rule-based methods for generating paired training texts, which limited performance. \n",
      "\n",
      "JoLT directly aligns time-series and text representations using a Querying Transformer (Q-Former), addressing the limitations of previous methods that convert time-series into images or graphs.  \n",
      "\n",
      "Key innovations include:\n",
      "\n",
      "* **Direct Alignment:** JoLT directly works with time-series and text data, enabling a more natural and potentially more effective integration of the two modalities.\n",
      "* **Querying Transformer:** The Q-Former mechanism allows for selective extraction of time-series features based on textual prompts, enhancing the model's ability to focus on relevant information.\n",
      "* **Pre-training:** JoLT leverages pre-trained language models and a time-series encoder trained on masked reconstruction tasks, providing a strong foundation for learning joint representations.\n",
      "\n",
      "Experiments on a large electrocardiography (ECG) dataset demonstrate that JoLT outperforms state-of-the-art methods for medical time-series summarization and question answering, highlighting the effectiveness of directly modeling the relationship between time-series and text data for improving clinical interpretation tasks. This opens up new capabilities like automated ECG interpretation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "sinus rhythm position type normal left bundle branch block left\n",
      "hypertrophy possible 4.46 unconfirmed reportGround\n",
      "Truth\n",
      "sinus rhythm. normal ecgFine-tuned\n",
      "BLIP-2\n",
      "sinus rhythm left type left bundle branch block left hypertrophy\n",
      "possible 4.46 unconfirmed reportJoLT\n",
      "(OPT-2.7B)\n",
      "Table 1: Qualitative evaluation of the results generated by JoLT compared to fine-tuned BLIP-2.\n",
      "JoLT (with OPT-2.7B decoder) generates text summaries very similar to the ground truth, while the\n",
      "fine-tuned BLIP-2 got the base class correct, but incorrectly described the time-series sample.\n",
      "input text. The Q-Former is trained using three objectives: (1) a contrastive loss to align time-series\n",
      "and text representations by maximizing their mutual information, (2) a text generation loss to train\n",
      "the Q-Former to generate text conditioned on input time-series, and a (3) time-series text matching\n",
      "lossfor finer grained alignment between time-series and text representations.\n",
      "Generative Learning. In this stage, we finally connect the frozen time-series encoder and Q-\n",
      "Former, with the frozen decoder, to leverage its generative capability. The query embeddings serve as\n",
      "soft prompts to guide the decoder’s language generation. We train the model end-to-end using the\n",
      "causal language modeling loss.\n",
      "4 Case Study: ECG Interpretation\n",
      "Dataset. We conduct an experiment on the PTB-XL dataset [ 7] to evaluate JoLT ’s ability to\n",
      "generate meaningful clinical interpretations from ECG waveform data. The dataset comprises of\n",
      "21,837 12-lead, 10 seconds long ECG recordings collected from 18,885 patients. A subset of ECG\n",
      "recordings is paired with gold-standard clinical interpretation, which we use to train and fine-tune\n",
      "our model. The train, validation, and test sets contain 11,319, 1,636, and 1,650 samples of paired\n",
      "time-series and text, respectively.\n",
      "Experimental Setup. We compare JoLT with the state-of-the-art image captioning model BLIP-2\n",
      "as baseline. We use the Matplotlib package1to transform time-series into graphical images before\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper introduces JoLT, a novel neural method for jointly learning representations from time-series and text data.  Existing approaches often struggle to effectively integrate these modalities, either treating them independently or relying on rule-based systems that require significant domain expertise and may not generalize well across different domains. JoLT directly aligns time-series and text representations using a Querying Transformer (Q-Former), addressing the limitations of previous methods that convert time-series into images or graphs.  \n",
      "\n",
      "Key innovations include:\n",
      "\n",
      "* **Direct Alignment:** JoLT directly works with time-series and text data, enabling a more natural and potentially more effective integration of the two modalities.\n",
      "* **Querying Transformer:** The Q-Former mechanism allows for selective extraction of time-series features based on textual prompts, enhancing the model's ability to focus on relevant information.\n",
      "* **Pre-training:** JoLT leverages pre-trained language models and a time-series encoder trained on masked reconstruction tasks, providing a strong foundation for learning joint representations.\n",
      "\n",
      "The Q-Former is trained using three objectives: (1) a contrastive loss to align time-series and text representations by maximizing their mutual information, (2) a text generation loss to train the Q-Former to generate text conditioned on input time-series, and (3) a time-series text matching loss for finer grained alignment. \n",
      "\n",
      "Experiments on a large electrocardiography (ECG) dataset demonstrate that JoLT outperforms state-of-the-art methods for medical time-series summarization and question answering, showcasing its ability to generate meaningful clinical interpretations from ECG data. This opens up new capabilities like automated ECG interpretation. For example, JoLT surpasses fine-tuned BLIP-2 in accurately capturing both the class and specific features of ECG recordings. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "as baseline. We use the Matplotlib package1to transform time-series into graphical images before\n",
      "feeding them into BLIP-2 . We evaluate BLIP-2 in a zero-shot setting. Since the models are not\n",
      "pre-trained on medical data, we also compare JoLT with BLIP-2 fine-tuned on the PTB-XL dataset.\n",
      "We evaluate multiple metrics that are commonly used to evaluate text generation performance.\n",
      "We further run ablation experiments to evaluate the impact of the decoder on our model’s performance.\n",
      "Specifically, we evaluate different architectures (e.g. GPT-2-Large versus OPT), of different sizes\n",
      "(e.g. OPT-2.7B versus OPT-6.7B ), and pre-trained on different data (e.g. BioGPT-Large versus\n",
      "GPT-2-Large ). Beyond ECG interpretation, we also run preliminary experiments to explore our\n",
      "JoLT ’s ability to solve multiple choice questions conditioned on time-series, on the PTB-XL dataset.\n",
      "To the best of our knowledge, this unique but important problem has not been explored in prior work.\n",
      "We compare JoLT against BiomedCLIP [13] as a baseline.\n",
      "Tables 1, 2, 3, and 4 summarize the results of our experiments. Below, we highlight some key\n",
      "observations.\n",
      "Domain-specific fine-tuning is critical for clinical waveform interpretation. Poor performance\n",
      "of off-the-shelf BLIP-2 with respect to its fine-tuned counterpart shows the need for domain-specific\n",
      "fine-tuning, at least within the clinical domain. This motivates the need for publicly available large\n",
      "paired time-series and text datasets and models.\n",
      "1https://pypi.org/project/matplotlib/\n",
      "3\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper introduces JoLT, a novel neural method for jointly learning representations from time-series and text data. Existing approaches often struggle to effectively integrate these modalities, either treating them independently or relying on rule-based systems that require significant domain expertise and may not generalize well across different domains.  JoLT directly aligns time-series and text representations using a Querying Transformer (Q-Former), addressing the limitations of previous methods that convert time-series into images or graphs.  \n",
      "\n",
      "Key innovations include:\n",
      "\n",
      "* **Direct Alignment:** JoLT directly works with time-series and text data, enabling a more natural and potentially more effective integration of the two modalities.\n",
      "* **Querying Transformer:** The Q-Former mechanism allows for selective extraction of time-series features based on textual prompts, enhancing the model's ability to focus on relevant information.\n",
      "* **Pre-training:** JoLT leverages pre-trained language models and a time-series encoder trained on masked reconstruction tasks, providing a strong foundation for learning joint representations.\n",
      "\n",
      "The Q-Former is trained using three objectives: (1) a contrastive loss to align time-series and text representations by maximizing their mutual information, (2) a text generation loss to train the Q-Former to generate text conditioned on input time-series, and (3) a time-series text matching loss for finer grained alignment. \n",
      "\n",
      "Experiments on a large electrocardiography (ECG) dataset demonstrate that JoLT outperforms state-of-the-art methods, including fine-tuned BLIP-2, for medical time-series summarization and question answering.  This showcases its ability to generate meaningful clinical interpretations from ECG data and open up new capabilities like automated ECG interpretation.  Beyond ECG interpretation, preliminary experiments on the PTB-XL dataset explore JoLT's ability to solve multiple choice questions conditioned on time-series. \n",
      "\n",
      "\n",
      "These results highlight the importance of domain-specific fine-tuning for clinical waveform interpretation and the need for publicly available large paired time-series and text datasets. \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Model Fine-tuned Rouge-1 Rouge-2 Rouge-LMETEOR BLEURTR P F 1 R P F 1 R P F 1\n",
      "BLIP-2 × 0.014 0.027 0.016 0.000 0.001 0.001 0.014 0.026 0.016 0.048 -1.283\n",
      "BLIP-2 ✓ 0.217 0.343 0.227 0.100 0.193 0.107 0.215 0.341 0.225 0.202 -0.930\n",
      "JoLT (OPT-2.7B) ✓ 0.404 0.528 0.436 0.277 0.355 0.295 0.403 0.526 0.435 0.414 -0.502\n",
      "Table 2: JoLT (with OPT-2.7B decoder) outperforms zero-shot and fine-tuned BLIP-2 baselines\n",
      "for the ECG interpretation task on the PTB-XL dataset. R,P, and F1denote the Recall, Precision,\n",
      "and F 1score.\n",
      "Decoder Rouge-1 Rouge-2 Rouge-LMETEOR BLEURTR P F 1 R P F 1 R P F 1\n",
      "OPT-2.7B 0.404 0.528 0.436 0.277 0.355 0.295 0.403 0.526 0.435 0.414 -0.502\n",
      "OPT-6.7B 0.400 0.518 0.429 0.277 0.350 0.294 0.399 0.517 0.428 0.408 -0.499\n",
      "GPT-2 0.113 0.420 0.169 0.020 0.080 0.029 0.113 0.419 0.168 0.017 -0.885\n",
      "BioGPT 0.107 0.342 0.153 0.022 0.072 0.031 0.107 0.342 0.153 0.212 -1.122\n",
      "BioMedLM 0.118 0.390 0.164 0.003 0.009 0.004 0.112 0.380 0.158 0.136 -1.079\n",
      "Table 3: JoLT with OPT-2.7B decoder outperforms JoLT with other decoders with different pre-\n",
      "trained data, different sizes, and different architectures on the ECG interpretation task on the PTB-XL\n",
      "dataset. R,P, and F1denote the Recall, Precision, and F 1score.\n",
      "Model Accuracy Weighted Precision Weighted Recall Weighted F1Score\n",
      "BiomedCLIP 0.11 0.57 0.11 0.02\n",
      "JoLT (OPT-2.7B) 0.54 0.31 0.54 0.4\n",
      "Table 4: Time-series conditioned multiple-choice question answering results on accuracy, precision,\n",
      "recall and F1-score. JoLT substantially outperforms BiomedCLIP , when given the prompt “ Which of\n",
      "the following five diagnostic classes does the following ECG belong to? \". Weighted averages are\n",
      "measured with respect to the support for each class.\n",
      "Explicitly modeling time-series improves summarization performance. JoLT produces textual\n",
      "summaries which are closer to ground truth compared to BLIP-2. We believe that the difference in\n",
      "performance largely stems from JoLT ’s ability to capture salient time-series features.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper introduces JoLT, a novel neural method for jointly learning representations from time-series and text data. Existing approaches often struggle to effectively integrate these modalities, either treating them independently or relying on rule-based systems that require significant domain expertise and may not generalize well across different domains.  JoLT directly aligns time-series and text representations using a Querying Transformer (Q-Former), addressing the limitations of previous methods that convert time-series into images or graphs.  \n",
      "\n",
      "Key innovations include:\n",
      "\n",
      "* **Direct Alignment:** JoLT directly works with time-series and text data, enabling a more natural and potentially more effective integration of the two modalities.\n",
      "* **Querying Transformer:** The Q-Former mechanism allows for selective extraction of time-series features based on textual prompts, enhancing the model's ability to focus on relevant information.\n",
      "* **Pre-training:** JoLT leverages pre-trained language models and a time-series encoder trained on masked reconstruction tasks, providing a strong foundation for learning joint representations.\n",
      "\n",
      "The Q-Former is trained using three objectives: (1) a contrastive loss to align time-series and text representations by maximizing their mutual information, (2) a text generation loss to train the Q-Former to generate text conditioned on input time-series, and (3) a time-series text matching loss for finer grained alignment. \n",
      "\n",
      "Experiments on a large electrocardiography (ECG) dataset demonstrate that JoLT outperforms state-of-the-art methods, including fine-tuned BLIP-2, for medical time-series summarization and question answering.  This showcases its ability to generate meaningful clinical interpretations from ECG data and open up new capabilities like automated ECG interpretation. JoLT achieves this by explicitly modeling time-series information, leading to textual summaries closer to ground truth compared to BLIP-2.  \n",
      "\n",
      "Beyond ECG interpretation, preliminary experiments on the PTB-XL dataset explore JoLT's ability to solve multiple choice questions conditioned on time-series.  These results highlight the importance of domain-specific fine-tuning for clinical waveform interpretation and the need for publicly available large paired time-series and text datasets. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "performance largely stems from JoLT ’s ability to capture salient time-series features.\n",
      "Time-series and text joint modeling helps improve upon baselines for multi-class question\n",
      "answering. JoLT outperforms BiomedCLIP on time-series conditioned multiple choice question\n",
      "answering problem. However, we note that both the models perform poorly on this task. We believe\n",
      "that future work should carefully look to improve our model’s performance on this important task,\n",
      "with both models overconfidently predicting the majority class.\n",
      "Broken assumptions. For our ablation experiments, we hypothesized that: (1) Language models\n",
      "with more parameters will be better at text generation than smaller models, and (2) models pre-trained\n",
      "on clinical data ( BioGPT ,BiomedLM ) will be better than those trained on general text data ( GPT-2 ).\n",
      "However, our experiments did not support any of these hypothesis. The former can be partly explained\n",
      "by the fact that clinical interpretations are terse and do not require fluent large language models. We\n",
      "believe that further experiments are necessary to conclusively accept or reject the latter hypothesis.\n",
      "5 Conclusion and Future Work\n",
      "In this work, we introduced JoLT , a jointly model text and time-series data with a focus on ECG\n",
      "interpretation. We evaluated our model against state-of-the-art image captioning models in the context\n",
      "of clinical summarization. It’s worth noting a crucial aspect of our approach: the encoder of JoLT\n",
      "was pre-trained using a relatively small set of time-series data from the PTB-XL dataset. We posit\n",
      "that extending this pre-training phase to include a large amount of time-series data is likely to further\n",
      "improve its performance. Additionally, it’s important to acknowledge that the evaluation of these\n",
      "models presents significant challenges, as discussed in prior research [ 14]. Therefore, future research\n",
      "endeavors should aim to comprehensively and robustly assess the capabilities of such models in\n",
      "clinical applications.\n",
      "4\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here's a refined summary incorporating the new context:\n",
      "\n",
      "This paper introduces JoLT, a novel neural method for jointly learning representations from time-series and text data.  Existing approaches often struggle to effectively integrate these modalities, either treating them independently or relying on rule-based systems that require significant domain expertise and may not generalize well.  JoLT directly aligns time-series and text representations using a Querying Transformer (Q-Former), addressing the limitations of previous methods that convert time-series into images or graphs.\n",
      "\n",
      "Key innovations include:\n",
      "\n",
      "* **Direct Alignment:** JoLT directly works with time-series and text data, enabling a more natural and potentially more effective integration of the two modalities.\n",
      "* **Querying Transformer:** The Q-Former mechanism allows for selective extraction of time-series features based on textual prompts, enhancing the model's ability to focus on relevant information.\n",
      "* **Pre-training:** JoLT leverages pre-trained language models and a time-series encoder trained on masked reconstruction tasks, providing a strong foundation for learning joint representations.\n",
      "\n",
      "The Q-Former is trained using three objectives: (1) a contrastive loss to align time-series and text representations by maximizing their mutual information, (2) a text generation loss to train the Q-Former to generate text conditioned on input time-series, and (3) a time-series text matching loss for finer grained alignment. \n",
      "\n",
      "Experiments on a large electrocardiography (ECG) dataset demonstrate that JoLT outperforms state-of-the-art methods, including fine-tuned BLIP-2, for medical time-series summarization and question answering.  This showcases its ability to generate meaningful clinical interpretations from ECG data and open up new capabilities like automated ECG interpretation. JoLT's performance stems from its ability to capture salient time-series features. \n",
      "\n",
      "While promising, future work will focus on:\n",
      "\n",
      "* **Expanding pre-training data:** Utilizing a larger dataset for time-series pre-training is expected to further improve JoLT's performance.\n",
      "* **Addressing challenges in evaluation:** Developing robust and comprehensive evaluation methods for models in clinical applications is crucial.\n",
      "* **Improving performance on multiple choice question answering:**  While JoLT shows promise, its performance on this task requires further investigation and improvement. \n",
      "* **Exploring the impact of model size and pre-training data:**  Ablation experiments suggest that the relationship between model size, pre-training data (clinical vs. general), and performance may be more complex than initially hypothesized.\n",
      "\n",
      "\n",
      "\n",
      "This refined summary incorporates the findings from the ablation experiments and future work directions, providing a more complete picture of JoLT's capabilities and limitations.\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "6 Acknowledgments\n",
      "This work was partially supported by the U.S. Army Research Office and the U.S. Army Futures\n",
      "Command under contract W911NF-20-F-0020.\n",
      "References\n",
      "[1]Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image\n",
      "pre-training with frozen image encoders and large language models. In ICML , 2023.\n",
      "[2]Anthony Colas, Mehrdad Alvandipour, and Daisy Zhe Wang. GAP: A graph-aware lan-\n",
      "guage model framework for knowledge graph-to-text generation. In Proceedings of the 29th\n",
      "International Conference on Computational Linguistics , pages 5755–5769, Gyeongju, Repub-\n",
      "lic of Korea, October 2022. International Committee on Computational Linguistics. URL\n",
      "https://aclanthology.org/2022.coling-1.506 .\n",
      "[3]Rong Ye, Wenxian Shi, Hao Zhou, Zhongyu Wei, and Lei Li. Variational template machine for\n",
      "data-to-text generation. In International Conference on Learning Representations , 2020. URL\n",
      "https://openreview.net/forum?id=HkejNgBtPB .\n",
      "[4]Mandar Sharma, Ajay Gogineni, and Naren Ramakrishnan. Innovations in neural data-to-text\n",
      "generation: A survey, 2023.\n",
      "[5]Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\n",
      "Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n",
      "5b: An open large-scale dataset for training next generation image-text models. Advances in\n",
      "Neural Information Processing Systems , 35:25278–25294, 2022.\n",
      "[6]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\n",
      "Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\n",
      "Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\n",
      "Proceedings, Part V 13 , pages 740–755. Springer, 2014.\n",
      "[7]Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Dieter Kreiseler, Fatima I. Lunze,\n",
      "Wojciech Samek, and Tobias Schaeffter. Ptb-xl, a large publicly available electrocardiography\n",
      "dataset. Scientific Data , 2020. doi: https://doi.org/10.1038/s41597-020-0495-6.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper introduces JoLT, a novel neural method for jointly learning representations from time-series and text data.  Existing approaches often struggle to effectively integrate these modalities, either treating them independently or relying on rule-based systems that require significant domain expertise and may not generalize well.  JoLT directly aligns time-series and text representations using a Querying Transformer (Q-Former), addressing the limitations of previous methods that convert time-series into images or graphs.\n",
      "\n",
      "Key innovations include:\n",
      "\n",
      "* **Direct Alignment:** JoLT directly works with time-series and text data, enabling a more natural and potentially more effective integration of the two modalities.\n",
      "* **Querying Transformer:** The Q-Former mechanism allows for selective extraction of time-series features based on textual prompts, enhancing the model's ability to focus on relevant information.\n",
      "* **Pre-training:** JoLT leverages pre-trained language models and a time-series encoder trained on masked reconstruction tasks, providing a strong foundation for learning joint representations.\n",
      "\n",
      "The Q-Former is trained using three objectives: (1) a contrastive loss to align time-series and text representations by maximizing their mutual information, (2) a text generation loss to train the Q-Former to generate text conditioned on input time-series, and (3) a time-series text matching loss for finer grained alignment. \n",
      "\n",
      "Experiments on the large publicly available PTB-XL electrocardiography (ECG) dataset demonstrate that JoLT outperforms state-of-the-art methods, including fine-tuned BLIP-2, for medical time-series summarization and question answering.  This showcases its ability to generate meaningful clinical interpretations from ECG data and open up new capabilities like automated ECG interpretation. JoLT's performance stems from its ability to capture salient time-series features. \n",
      "\n",
      "While promising, future work will focus on:\n",
      "\n",
      "* **Expanding pre-training data:** Utilizing a larger dataset for time-series pre-training is expected to further improve JoLT's performance.\n",
      "* **Addressing challenges in evaluation:** Developing robust and comprehensive evaluation methods for models in clinical applications is crucial.\n",
      "* **Improving performance on multiple choice question answering:**  While JoLT shows promise, its performance on this task requires further investigation and improvement. \n",
      "* **Exploring the impact of model size and pre-training data:**  Ablation experiments suggest that the relationship between model size, pre-training data (clinical vs. general), and performance may be more complex than initially hypothesized.\n",
      "\n",
      "\n",
      "\n",
      "This refined summary incorporates the findings from the paper, including the use of the PTB-XL dataset, and future work directions, providing a more complete picture of JoLT's capabilities and limitations. \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "dataset. Scientific Data , 2020. doi: https://doi.org/10.1038/s41597-020-0495-6.\n",
      "[8]Siddharth Biswal, Cao Xiao, M. Brandon Westover, and Jimeng Sun. Eegtotext: Learning\n",
      "to write medical reports from eeg recordings. In Finale Doshi-Velez, Jim Fackler, Ken Jung,\n",
      "David Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiens, editors, Proceedings of the\n",
      "4th Machine Learning for Healthcare Conference , volume 106 of Proceedings of Machine\n",
      "Learning Research , pages 513–531. PMLR, 09–10 Aug 2019. URL https://proceedings.\n",
      "mlr.press/v106/biswal19a.html .\n",
      "[9]Steffen Pauws, Albert Gatt, Emiel Krahmer, and Ehud Reiter. Making effective use of healthcare\n",
      "data using data-to-text technology, 2018.\n",
      "[10] Jonathan Harris and Mohammed J. Zaki. Towards neural numeric-to-text generation from\n",
      "temporal personal health data, 2022.\n",
      "[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
      "Ł ukasz Kaiser, and Illia Polosukhin. Transformers: Attention is all you need. In I. Guyon,\n",
      "U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, ed-\n",
      "itors, Advances in Neural Information Processing Systems , volume 30. Curran Associates,\n",
      "Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/\n",
      "3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .\n",
      "[12] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\n",
      "Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam\n",
      "Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke\n",
      "Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.\n",
      "5\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This paper introduces JoLT, a novel neural method for jointly learning representations from time-series and text data.  Existing approaches often struggle to effectively integrate these modalities, either treating them independently or relying on rule-based systems that require significant domain expertise and may not generalize well. JoLT directly aligns time-series and text representations using a Querying Transformer (Q-Former), addressing the limitations of previous methods that convert time-series into images or graphs.\n",
      "\n",
      "Key innovations include:\n",
      "\n",
      "* **Direct Alignment:** JoLT directly works with time-series and text data, enabling a more natural and potentially more effective integration of the two modalities.\n",
      "* **Querying Transformer:** The Q-Former mechanism allows for selective extraction of time-series features based on textual prompts, enhancing the model's ability to focus on relevant information.\n",
      "* **Pre-training:** JoLT leverages pre-trained language models and a time-series encoder trained on masked reconstruction tasks, providing a strong foundation for learning joint representations.\n",
      "\n",
      "The Q-Former is trained using three objectives: (1) a contrastive loss to align time-series and text representations by maximizing their mutual information, (2) a text generation loss to train the Q-Former to generate text conditioned on input time-series, and (3) a time-series text matching loss for finer grained alignment. \n",
      "\n",
      "Experiments on the large publicly available PTB-XL electrocardiography (ECG) dataset demonstrate that JoLT outperforms state-of-the-art methods, including fine-tuned BLIP-2, for medical time-series summarization and question answering.  This showcases its ability to generate meaningful clinical interpretations from ECG data and open up new capabilities like automated ECG interpretation. JoLT's performance stems from its ability to capture salient time-series features. \n",
      "\n",
      "While promising, future work will focus on:\n",
      "\n",
      "* **Expanding pre-training data:** Utilizing a larger dataset for time-series pre-training is expected to further improve JoLT's performance.\n",
      "* **Addressing challenges in evaluation:** Developing robust and comprehensive evaluation methods for models in clinical applications is crucial.\n",
      "* **Improving performance on multiple choice question answering:**  While JoLT shows promise, its performance on this task requires further investigation and improvement. \n",
      "* **Exploring the impact of model size and pre-training data:**  Ablation experiments suggest that the relationship between model size, pre-training data (clinical vs. general), and performance may be more complex than initially hypothesized. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "[13] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh\n",
      "Rao, Mu Wei, Naveen Valluri, Cliff Wong, Matthew P. Lungren, Tristan Naumann, and Hoifung\n",
      "Poon. Large-scale domain-specific pretraining for biomedical vision-language processing, 2023.\n",
      "[14] Michael Wornow, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming,\n",
      "Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. The shaky foundations of large lan-\n",
      "guage models and foundation models for electronic health records. npj Digital Medicine ,\n",
      "6(1):135, Jul 2023. ISSN 2398-6352. doi: 10.1038/s41746-023-00879-8. URL https:\n",
      "//doi.org/10.1038/s41746-023-00879-8 .\n",
      "6\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"refine\", verbose=True)\n",
    "summary = chain.run(splitted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This paper introduces JoLT, a novel neural method for jointly learning representations from time-series and text data. Existing approaches often struggle to effectively integrate these modalities, either treating them independently or relying on rule-based systems that require significant domain expertise and may not generalize well.  JoLT directly aligns time-series and text representations using a Querying Transformer (Q-Former), addressing the limitations of previous methods that convert time-series into images or graphs.\\n\\nKey innovations include:\\n\\n* **Direct Alignment:** JoLT directly works with time-series and text data, enabling a more natural and potentially more effective integration of the two modalities.\\n* **Querying Transformer:** The Q-Former mechanism allows for selective extraction of time-series features based on textual prompts, enhancing the model's ability to focus on relevant information.\\n* **Pre-training:** JoLT leverages pre-trained language models and a time-series encoder trained on masked reconstruction tasks, providing a strong foundation for learning joint representations.\\n\\nThe Q-Former is trained using three objectives: (1) a contrastive loss to align time-series and text representations by maximizing their mutual information, (2) a text generation loss to train the Q-Former to generate text conditioned on input time-series, and (3) a time-series text matching loss for finer grained alignment. \\n\\nExperiments on the large publicly available PTB-XL electrocardiography (ECG) dataset demonstrate that JoLT outperforms state-of-the-art methods, including fine-tuned BLIP-2, for medical time-series summarization and question answering.  This showcases its ability to generate meaningful clinical interpretations from ECG data and open up new capabilities like automated ECG interpretation.  JoLT's performance stems from its ability to capture salient time-series features. \\n\\nWhile promising, future work will focus on:\\n\\n* **Expanding pre-training data:** Utilizing a larger dataset for time-series pre-training is expected to further improve JoLT's performance. This aligns with the call for large-scale domain-specific pre-training in biomedical vision-language processing (Zhang et al., 2023).\\n* **Addressing challenges in evaluation:** Developing robust and comprehensive evaluation methods for models in clinical applications is crucial. This is particularly important given the concerns raised about the shaky foundations of large language models in healthcare (Wornow et al., 2023).\\n* **Improving performance on multiple choice question answering:**  While JoLT shows promise, its performance on this task requires further investigation and improvement. \\n* **Exploring the impact of model size and pre-training data:**  Ablation experiments suggest that the relationship between model size, pre-training data (clinical vs. general), and performance may be more complex than initially hypothesized. \\n\\n\\n\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
