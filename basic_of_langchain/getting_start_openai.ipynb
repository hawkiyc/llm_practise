{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001AD18332C50>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001AD1468B390>, root_client=<openai.OpenAI object at 0x000001AD18333D10>, root_async_client=<openai.AsyncOpenAI object at 0x000001AD188A9190>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model = 'gpt-4o-mini')\n",
    "llm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chat & get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The transformer architecture is a type of neural network architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. It was designed primarily for natural language processing (NLP) tasks, but its principles have been applied to various domains, including computer vision and reinforcement learning.\\n\\n### Key Characteristics of the Transformer Architecture:\\n\\n1. **Self-Attention Mechanism:**\\n   - The core innovation of transformers is the self-attention mechanism, which allows the model to weigh the importance of different words in a sentence when encoding them. This means that each word can attend to all other words, allowing it to capture contextual relationships more effectively than traditional recurrent neural networks (RNNs).\\n\\n2. **Multi-Head Attention:**\\n   - Transformers use multiple attention heads to capture different types of relationships and dependencies within the input data. Each head can focus on different parts of the input, enabling the model to learn richer representations.\\n\\n3. **Positional Encoding:**\\n   - Since the transformer model does not inherently understand the order of the input sequence (unlike RNNs), it incorporates positional encodings to represent the position of each token in the sequence. This helps the model maintain the sequential information of the input data.\\n\\n4. **Feed-Forward Networks:**\\n   - After the attention layers, the transformer includes feed-forward neural networks that process each position independently, further transforming the representations.\\n\\n5. **Layer Normalization and Residual Connections:**\\n   - Transformers utilize layer normalization and residual connections (also called skip connections) to stabilize training and improve convergence. These help in propagating gradients back through the network during training.\\n\\n6. **Encoder-Decoder Structure:**\\n   - The original transformer architecture consists of two main components: an encoder and a decoder. The encoder processes the input sequence and generates a set of continuous representations, while the decoder takes these representations and generates the output sequence, usually one token at a time.\\n\\n7. **Scalability and Parallelization:**\\n   - Transformers are highly parallelizable, allowing them to be trained on large datasets more efficiently compared to RNNs, which process sequences sequentially.\\n\\n### Applications:\\nTransformers have revolutionized NLP and are the backbone of many state-of-the-art models, such as BERT, GPT, T5, and others. They are also being adapted for various tasks, including image processing (e.g., Vision Transformers), time-series forecasting, and more.\\n\\n### Conclusion:\\nThe transformer architecture has become a foundational model in deep learning due to its effectiveness, scalability, and versatility. Its self-attention mechanism and parallel processing capabilities allow it to capture complex patterns in data, making it a powerful tool for various machine learning tasks.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 547, 'prompt_tokens': 12, 'total_tokens': 559, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-cdc6dce9-b84b-4bd4-8529-a491527ed38f-0', usage_metadata={'input_tokens': 12, 'output_tokens': 547, 'total_tokens': 559})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = llm.invoke(\"What is transformer architecture?\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The transformer architecture is a type of neural network architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. It was designed primarily for natural language processing (NLP) tasks, but its principles have been applied to various domains, including computer vision and reinforcement learning.\\n\\n### Key Characteristics of the Transformer Architecture:\\n\\n1. **Self-Attention Mechanism:**\\n   - The core innovation of transformers is the self-attention mechanism, which allows the model to weigh the importance of different words in a sentence when encoding them. This means that each word can attend to all other words, allowing it to capture contextual relationships more effectively than traditional recurrent neural networks (RNNs).\\n\\n2. **Multi-Head Attention:**\\n   - Transformers use multiple attention heads to capture different types of relationships and dependencies within the input data. Each head can focus on different parts of the input, enabling the model to learn richer representations.\\n\\n3. **Positional Encoding:**\\n   - Since the transformer model does not inherently understand the order of the input sequence (unlike RNNs), it incorporates positional encodings to represent the position of each token in the sequence. This helps the model maintain the sequential information of the input data.\\n\\n4. **Feed-Forward Networks:**\\n   - After the attention layers, the transformer includes feed-forward neural networks that process each position independently, further transforming the representations.\\n\\n5. **Layer Normalization and Residual Connections:**\\n   - Transformers utilize layer normalization and residual connections (also called skip connections) to stabilize training and improve convergence. These help in propagating gradients back through the network during training.\\n\\n6. **Encoder-Decoder Structure:**\\n   - The original transformer architecture consists of two main components: an encoder and a decoder. The encoder processes the input sequence and generates a set of continuous representations, while the decoder takes these representations and generates the output sequence, usually one token at a time.\\n\\n7. **Scalability and Parallelization:**\\n   - Transformers are highly parallelizable, allowing them to be trained on large datasets more efficiently compared to RNNs, which process sequences sequentially.\\n\\n### Applications:\\nTransformers have revolutionized NLP and are the backbone of many state-of-the-art models, such as BERT, GPT, T5, and others. They are also being adapted for various tasks, including image processing (e.g., Vision Transformers), time-series forecasting, and more.\\n\\n### Conclusion:\\nThe transformer architecture has become a foundational model in deep learning due to its effectiveness, scalability, and versatility. Its self-attention mechanism and parallel processing capabilities allow it to capture complex patterns in data, making it a powerful tool for various machine learning tasks.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a very experienced cardio-doctor. Please answer patients based on the questions.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are a very experienced cardio-doctor. Please answer patients based on the questions.\"),\n",
    "     (\"user\", \"{input}\")])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atrial fibrillation (AFib) is a common type of irregular heartbeat (arrhythmia) that can lead to various complications, including stroke and heart-related issues. In AFib, the heart's two upper chambers (the atria) experience chaotic electrical signals, causing them to quiver instead of contracting effectively. This can lead to an irregular and often rapid heart rate.\n",
      "\n",
      "The causes of AFib can vary and may include:\n",
      "\n",
      "1. **Heart Conditions**: Conditions such as hypertension (high blood pressure), coronary artery disease, heart valve disorders, or heart failure can increase the risk of developing AFib.\n",
      "\n",
      "2. **Age**: The risk of AFib increases with age, particularly after age 60.\n",
      "\n",
      "3. **Other Medical Conditions**: Conditions such as hyperthyroidism, diabetes, and chronic lung diseases can contribute to the development of AFib.\n",
      "\n",
      "4. **Lifestyle Factors**: Excessive alcohol consumption, obesity, smoking, and lack of physical activity can increase the risk.\n",
      "\n",
      "5. **Genetics**: A family history of AFib may increase the likelihood of developing the condition.\n",
      "\n",
      "6. **Stress and Illness**: Physical stress (such as surgery or infection) and emotional stress can trigger AFib episodes.\n",
      "\n",
      "7. **Sleep Apnea**: This sleep disorder is known to be associated with an increased risk of AFib.\n",
      "\n",
      "If you have concerns about AFib or its symptoms, such as palpitations, shortness of breath, or fatigue, it's important to consult with a healthcare professional for proper evaluation and management.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|llm\n",
    "answer2 = chain.invoke({\"input\": \"What is afib and what cause it?\"})\n",
    "print(answer2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(answer2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "chain2 = prompt|llm|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atrial fibrillation (AFib) is a common type of arrhythmia, or irregular heartbeat. In AFib, the heart's two upper chambers (the atria) experience chaotic electrical signals, leading to an irregular and often rapid heart rate. This can prevent the heart from pumping blood effectively, which may increase the risk of blood clots, stroke, heart failure, and other heart-related complications.\n",
      "\n",
      "The causes of AFib can vary and may include:\n",
      "\n",
      "1. **Heart-related conditions**: Such as high blood pressure, coronary artery disease, heart valve diseases, or previous heart surgery.\n",
      "2. **Other medical conditions**: Including hyperthyroidism, diabetes, and sleep apnea.\n",
      "3. **Lifestyle factors**: Such as excessive alcohol or caffeine consumption, smoking, obesity, and lack of physical activity.\n",
      "4. **Age**: The risk of developing AFib increases with age.\n",
      "5. **Family history**: A history of AFib in the family can increase your risk.\n",
      "6. **Stress or illness**: Acute illnesses, infections, or significant emotional stress can trigger AFib episodes.\n",
      "\n",
      "If you suspect you have AFib or are experiencing symptoms like palpitations, shortness of breath, or fatigue, it's essential to consult with a healthcare provider for an accurate diagnosis and appropriate management.\n"
     ]
    }
   ],
   "source": [
    "answer3 = chain2.invoke({\"input\": \"What is afib and what cause it?\"})\n",
    "print(answer3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
